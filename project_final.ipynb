{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект: Предиктивная модель оценки стоимости недвижимости\n",
    "\n",
    "## 1. Описание кейса и бизнес-постановка задачи\n",
    "К нам обратился представитель крупного агентства недвижимости.\n",
    "\n",
    "**Проблема:** Риелторы тратят слишком много времени на ручную сортировку объявлений и поиск выгодных предложений. Скорость реакции и качество ручной оценки уступают конкурентам, что негативно сказывается на финансовых показателях агентства.\n",
    "\n",
    "**Цель проекта:**\n",
    "\n",
    "Разработать сервис (модель машинного обучения), который позволит **автоматически предсказывать рыночную стоимость объектов недвижимости** на основе их характеристик и истории предложений. Это поможет риелторам быстрее находить недооцененные варианты и точнее формировать предложения для клиентов.\n",
    "\n",
    "## 2. Поставленные задачи\n",
    "Для достижения цели необходимо решить следующие технические и аналитические задачи:\n",
    "\n",
    "1.  **Разведочный анализ данных (EDA) и Очистка:**\n",
    "    *   Провести парсинг сложных вложенных структур (JSON) в признаках `homeFacts` и `schools`.\n",
    "    *   Выявить и обработать аномальные выбросы (ошибки ввода площади, нереалистичные цены), которые могут исказить обучение.\n",
    "    *   Проанализировать распределения и корреляции признаков.\n",
    "\n",
    "2.  **Feature Engineering (Проектирование признаков):**\n",
    "    *   Восстановить и использовать географические данные (`latitude`, `longitude`, `zipcode`).\n",
    "    *   Применить продвинутые методы кодирования категорий (Target Encoding) для признаков с высокой кардинальностью.\n",
    "    *   Сформировать новые информативные признаки (возраст дома, расстояние до школ, средний рейтинг района).\n",
    "\n",
    "3.  **Построение ML-пайплайна:**\n",
    "    *   Разработать воспроизводимый `sklearn.pipeline`, включающий обработку пропусков, масштабирование и кодирование.\n",
    "    *   **Критически важно:** Исключить утечку данных (Data Leakage) — проводить все трансформации строго после разделения на обучающую и тестовую выборки.\n",
    "\n",
    "4.  **Моделирование и Оптимизация:**\n",
    "    *   Построить Baseline-модель для оценки базового качества.\n",
    "    *   Провести автоматический подбор гиперпараметров с помощью **Optuna**.\n",
    "    *   Реализовать ансамблевые методы (**Stacking**) с использованием **GPU-ускорения** для достижения максимальной точности.\n",
    "\n",
    "5.  **Внедрение:**\n",
    "    *   Подготовить и сериализовать (сохранить) лучшую модель для дальнейшей интеграции в веб-сервис.\n",
    "\n",
    "## 3. Описание данных\n",
    "В работе используется [датасет](https://drive.google.com/file/d/11-ZNNIdcQ7TbT8Y0nsQ3Q0eiYQP__NIW/view?usp=share_link), содержащий ~377 000 записей о продаже недвижимости.\n",
    "Ключевые признаки:\n",
    "*   **Целевая переменная:** `target` (стоимость объекта недвижимости).\n",
    "*   **Характеристики:** площадь (`sqft`, `lotsize`), количество комнат (`beds`, `baths`), этажность, тип недвижимости.\n",
    "*   **Локация:** адрес, город, штат, почтовый индекс, координаты.\n",
    "*   **Инфраструктура:** данные о школах, рейтинги, расстояния."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. НАСТРОЙКА ОКРУЖЕНИЯ И ИМПОРТ БИБЛИОТЕК\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import datetime\n",
    "import time\n",
    "import joblib\n",
    "import ast\n",
    "import logging\n",
    "import platform\n",
    "import subprocess\n",
    "import psutil\n",
    "import json\n",
    "\n",
    "\n",
    "# Получаем количество ядер процессора\n",
    "cpu_count = os.cpu_count() or 1  # or 1 на случай, если функция вернет None\n",
    "\n",
    "# Логика:\n",
    "# 1. Берем все ядра, но оставляем одно свободным для системы (-1)\n",
    "# 2. Но не больше 6-8 (чтобы не взорвать память копиями данных)\n",
    "# 3. И не меньше 2 (чтобы хоть какая-то параллельность была)\n",
    "\n",
    "# При необходимости можно самостоятельно изменить количество потоков, SAFE_N_JOBS = -1 <- задействова все ядра\n",
    "SAFE_N_JOBS = max(2, min(8, cpu_count - 1))\n",
    "# SAFE_N_JOBS = -1\n",
    "\n",
    "print(f\"Количество ядер: {cpu_count}\")\n",
    "print(f\"Выбрано безопасное число потоков: {SAFE_N_JOBS}\")\n",
    "\n",
    "# --- Intel Acceleration (CPU) ---\n",
    "try:\n",
    "    from sklearnex import patch_sklearn\n",
    "    patch_sklearn()\n",
    "    print('Intel Extension (CPU) активирован')\n",
    "    # Подавление предупреждений от Intel Extension\n",
    "    warnings.filterwarnings(\"ignore\", module=\"daal4py\")\n",
    "    warnings.filterwarnings(\"ignore\", message=\".*Threading.*\")\n",
    "except (ImportError, Exception):\n",
    "    print('Intel Extension не найден, работаем на стандартном CPU')\n",
    "\n",
    "\n",
    "# Флаг пользователя: Хотим ли мы использовать GPU (NVIDIA), если она есть?\n",
    "ENABLE_GPU_COMPUTING = True \n",
    "\n",
    "GPU_NAME = \"CPU\"\n",
    "REAL_GPU_AVAILABLE = False\n",
    "DEVICE_TYPE = \"cpu\" # 'cuda', 'mps' (mac), или 'cpu'\n",
    "\n",
    "if ENABLE_GPU_COMPUTING:\n",
    "    # -----------------------------------------------------------\n",
    "    # ПОПЫТКА 1: Проверяем PyTorch (Самый надежный способ)\n",
    "    # -----------------------------------------------------------\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            REAL_GPU_AVAILABLE = True\n",
    "            GPU_NAME = torch.cuda.get_device_name(0)\n",
    "            DEVICE_TYPE = \"cuda\"\n",
    "        elif torch.backends.mps.is_available(): # Проверка для Mac M1/M2/M3\n",
    "            REAL_GPU_AVAILABLE = True\n",
    "            GPU_NAME = \"Apple Silicon GPU (MPS)\"\n",
    "            DEVICE_TYPE = \"mps\"\n",
    "    except ImportError:\n",
    "        pass # Torch нет, идем дальше\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # ПОПЫТКА 2: Если Torch нет, пробуем nvidia-smi (Windows/Linux)\n",
    "    # -----------------------------------------------------------\n",
    "    if not REAL_GPU_AVAILABLE:\n",
    "        try:\n",
    "            # Флаг shell=True нужен для Windows, чтобы найти команду\n",
    "            # Но на Linux он не обязателен. Делаем универсально:\n",
    "            cmd = \"nvidia-smi -L\"\n",
    "            result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "            \n",
    "            if result.returncode == 0 and \"GPU\" in result.stdout:\n",
    "                REAL_GPU_AVAILABLE = True\n",
    "                # Парсим название первой карты\n",
    "                GPU_NAME = result.stdout.split(':')[1].split('(')[0].strip()\n",
    "                DEVICE_TYPE = \"cuda\"\n",
    "        except Exception:\n",
    "            pass # Не получилось вызвать консоль или нет драйверов\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # ПОПЫТКА 3: Проверка на Mac (без Torch)\n",
    "    # -----------------------------------------------------------\n",
    "    if not REAL_GPU_AVAILABLE:\n",
    "        if platform.system() == 'Darwin' and platform.machine() == 'arm64':\n",
    "            # Косвенный признак, что мы на Apple Silicon\n",
    "            REAL_GPU_AVAILABLE = True \n",
    "            GPU_NAME = \"Apple Silicon (Detected via Platform)\"\n",
    "            DEVICE_TYPE = \"mps\"\n",
    "\n",
    "# ИТОГОВЫЙ ВЫВОД\n",
    "if REAL_GPU_AVAILABLE:\n",
    "    print(f'GPU активирована: {GPU_NAME} (Type: {DEVICE_TYPE})')\n",
    "else:\n",
    "    print(f'GPU не найдена или не настроена. Работаем на CPU.')\n",
    "\n",
    "# Глобальная переменная для ваших моделей\n",
    "USE_GPU = REAL_GPU_AVAILABLE\n",
    "\n",
    "# 3. ИМПОРТ DATA SCIENCE БИБЛИОТЕК\n",
    "\n",
    "# Работа с данными\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Визуализация\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Статистика (Тесты гипотез)\n",
    "from scipy import stats\n",
    "from scipy.stats import f_oneway, kruskal # <--- НУЖНО ДЛЯ ANOVA\n",
    "from scipy.stats import mannwhitneyu, normaltest, pearsonr, spearmanr\n",
    "import scikit_posthocs as sp # <--- НУЖНО ДЛЯ ТЕСТА ДАННА\n",
    "\n",
    "\n",
    "# Machine Learning (Sklearn Modules - для поддержки старого кода)\n",
    "from sklearn import (\n",
    "    model_selection,\n",
    "    preprocessing,\n",
    "    linear_model,\n",
    "    metrics,\n",
    "    tree,\n",
    "    ensemble,\n",
    "    feature_selection\n",
    ")\n",
    "\n",
    "# Machine Learning (Direct Imports - для пайплайнов)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    StackingRegressor, \n",
    "    GradientBoostingRegressor, \n",
    "    RandomForestRegressor, \n",
    "    HistGradientBoostingRegressor\n",
    ")\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFECV, SelectFromModel\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Boosting (работают и на CPU, и на GPU)\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# Инструменты ML\n",
    "import optuna\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "from category_encoders import TargetEncoder \n",
    "import pgeocode # Для геоданных\n",
    "\n",
    "\n",
    "# Формирование визуализации и представления датасета с помощью D-Tale (при необходимости)\n",
    "GENERATE_DTALE = True\n",
    "if GENERATE_DTALE:\n",
    "    import dtale\n",
    "    import dtale.app as dtale_app\n",
    "    import dtale.global_state as global_state\n",
    "\n",
    "# 4. НАСТРОЙКИ ЛОГИРОВАНИЯ И ОТОБРАЖЕНИЯ\n",
    "\n",
    "# Настройки Pandas и Plotly\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8') \n",
    "sns.set_style(\"whitegrid\")\n",
    "pio.renderers.default = 'notebook' # Для отображения в Jupyter / GitHub\n",
    "\n",
    "# Подавление предупреждений\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Логирование в файл\n",
    "if not os.path.exists('./logs'):\n",
    "    os.makedirs('logs')\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "log_filename = f\"logs/experiments_{timestamp}.log\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_filename,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%H:%M:%S',\n",
    "    force=True \n",
    ")\n",
    "\n",
    "# Фильтр шума от библиотек\n",
    "logging.getLogger('sklearnex').setLevel(logging.WARNING)\n",
    "logging.getLogger('optuna').setLevel(logging.WARNING)\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "logging.getLogger('ipykernel').setLevel(logging.ERROR)\n",
    "logging.getLogger('comm').setLevel(logging.ERROR)\n",
    "\n",
    "print(f\"Логирование настроено в файл: {log_filename}\")\n",
    "logging.info(\"Старт сессии\")\n",
    "\n",
    "# Глобальные константы\n",
    "\n",
    "# Настройки для воспроизводимости\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Путь к датасету\n",
    "DATASET_PATH = './data/data.csv.zip'  \n",
    "\n",
    "# Список для сбора результатов экспериментов\n",
    "EXPERIMENTS_DATA = [] \n",
    "\n",
    "\n",
    "# Отключаем специфичные предупреждения от Intel daal4py\n",
    "warnings.filterwarnings(\"ignore\", module=\"daal4py\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Threading.*backend is not supported.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*parallel backend is not supported by Extension for Scikit-learn.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_system_info():\n",
    "    \"\"\"Записывает характеристики железа и ОС в лог.\"\"\"\n",
    "    try:\n",
    "        # ОС и Python\n",
    "        os_info = f\"{platform.system()} {platform.release()} ({platform.version()})\"\n",
    "        python_info = sys.version.split()[0]\n",
    "        \n",
    "        # CPU\n",
    "        cpu_info = f\"{platform.processor()} ({os.cpu_count()} cores)\"\n",
    "        \n",
    "        # RAM\n",
    "        #import psutil\n",
    "        ram_bytes = psutil.virtual_memory().total\n",
    "        ram_gb = round(ram_bytes / (1024**3), 2)\n",
    "        \n",
    "        # GPU (используем уже определенную ранее логику или проверяем заново)\n",
    "        if 'torch' in sys.modules and torch.cuda.is_available():\n",
    "            gpu_info = torch.cuda.get_device_name(0)\n",
    "            vram = round(torch.cuda.get_device_properties(0).total_memory / (1024**3), 2)\n",
    "            gpu_str = f\"{gpu_info} ({vram} GB VRAM)\"\n",
    "        else:\n",
    "            gpu_str = \"CPU mode (No NVIDIA GPU detected)\"\n",
    "\n",
    "        # Формируем блок текста\n",
    "        info_msg = (\n",
    "            f\"\\n{'='*65}\\n\"\n",
    "            f\"SYSTEM CONFIGURATION\\n\"\n",
    "            f\"{'-'*65}\\n\"\n",
    "            f\"Date:   {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n",
    "            f\"OS:     {os_info}\\n\"\n",
    "            f\"Python: {python_info}\\n\"\n",
    "            f\"CPU:    {cpu_info}\\n\"\n",
    "            f\"RAM:    {ram_gb} GB\\n\"\n",
    "            f\"GPU:    {gpu_str}\\n\"\n",
    "            f\"{'='*65}\\n\"\n",
    "        )\n",
    "        \n",
    "        logging.info(info_msg)\n",
    "        print(\"Системная информация записана в лог.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Не удалось собрать системную информацию: {e}\")\n",
    "\n",
    "# Запускаем сразу после настройки логгера\n",
    "log_system_info()\n",
    "\n",
    "# Запоминаем время старта всего ноутбука\n",
    "TOTAL_START_TIME = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных, Разведочный анализ данных (EDA) и Очистка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e601f966-d03f-4edc-886f-ca3d511a8045",
    "_uuid": "569c301bd128f66f29b4d97c34171e4d1712015a",
    "id": "y7UAvo9Y7w33"
   },
   "outputs": [],
   "source": [
    "# Проверка наличия файла перед загрузкой\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    error_msg = f\"ОШИБКА: Файл данных не найден по пути: {DATASET_PATH}\"\n",
    "    logging.error(error_msg)\n",
    "    raise FileNotFoundError(error_msg)\n",
    "\n",
    "# Загрузка данных\n",
    "df_estate = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "# --- ЛОГИРОВАНИЕ СОСТОЯНИЯ ДАННЫХ ---\n",
    "rows, cols = df_estate.shape\n",
    "# Считаем реальную память (deep=True)\n",
    "memory_usage = df_estate.memory_usage(deep=True).sum() / (1024 * 1024) \n",
    "\n",
    "# Считаем пропуски\n",
    "missing_cells = df_estate.isnull().sum().sum()\n",
    "total_cells = rows * cols\n",
    "# Процент пропусков относительно ВСЕХ ячеек таблицы\n",
    "sparsity = (missing_cells / total_cells) * 100\n",
    "\n",
    "data_summary = (\n",
    "    f\"DATASET LOADED: {DATASET_PATH}\\n\"\n",
    "    f\"SHAPE: {rows} rows x {cols} columns | MEMORY: {memory_usage:.1f} MB\\n\"\n",
    "    f\"MISSING: {missing_cells} cells ({sparsity:.1f}% of all values)\"\n",
    ")\n",
    "\n",
    "print(data_summary)\n",
    "logging.info(data_summary)\n",
    "\n",
    "display(df_estate.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В датасете имеем 377185 разных объектов недвижимости описанных восемнадцатью признаками:\n",
    "\n",
    "1. status - статус продажи\n",
    "2. private pool - частный бассейн\n",
    "3. propertyType - тип недвижимости\n",
    "4. street - улица\n",
    "5. baths - число ванных комнат \n",
    "6. homeFacts - сведения о доме \n",
    "7. fireplace - тип отопления\n",
    "8. city - город\n",
    "9. schools - школы\n",
    "10. sqft - квадратный фут\n",
    "11. zipcode - почтовый индекс\n",
    "12. beds - число комнат\n",
    "13. state - штат\n",
    "14. stories - этажность\n",
    "15. mls-id - это код в их централизованной системе учёта предложений объектов недвижимости\n",
    "16. PrivatePool - частный бассейн\n",
    "17. MlsId - это код в их централизованной системе учёта предложений объектов недвижимости\n",
    "18. target - целевой признак, цена недвижимости\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Первичный анализ и очистка данных\n",
    "\n",
    "Проведем осмотр данных, проверим типы, наличие пропусков и дубликатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все признаки имеют тип object, в том числе целевой.  Кроме того, имеются пропуски во всех признаках. \n",
    "\n",
    "Сформилируем дальнейшую схему работы/обработки данных на следующем этапе:\n",
    "\n",
    "1. Привести целевой признак(target) к числовому типу\n",
    "2. Обработать пропуски в целевом признаке(target)\n",
    "3. Проанализировать и удалить неинформативные признаки\n",
    "4. Отдельно обработать информацию в признаках homeFacts и school, содержащие пул данных\n",
    "5. Обработать пропуски в остальных признаках\n",
    "6. Спроектировать новые признаки\n",
    "7. Визуализировать данные, найти зависимости, гипотезы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Размер датасета: {df_estate.shape}\")\n",
    "duplicates = df_estate.duplicated().sum()\n",
    "print(f\"Количество дубликатов: {duplicates} ({duplicates/len(df_estate):.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление дубликатов\n",
    "Удалим дубликаты, т.к. они могут исказить результаты анализа и обучения модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate = df_estate.drop_duplicates()\n",
    "print(f\"Размер после удаления дубликатов: {df_estate.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка целевой переменной (target)\n",
    "Целевая переменная сейчас в строковом формате (содержит '$' и ','). Преобразуем ее в число."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_currency(x):\n",
    "    if isinstance(x, str):\n",
    "        return x.replace('$', '').replace(',', '').replace('+', '')\n",
    "    return x\n",
    "\n",
    "df_estate['target_clean'] = df_estate['target'].apply(clean_currency)\n",
    "df_estate['target_clean'] = pd.to_numeric(df_estate['target_clean'], errors='coerce')\n",
    "# удаляем признак target\n",
    "df_estate = df_estate.drop(['target'], axis=1)\n",
    "\n",
    "print(f'Число пропусков в target_clean: {df_estate[\"target_clean\"].isnull().sum()}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеем 2878 пропусков в целевой переменной, что составляет менее 1% от общего объёма датасета. \n",
    "\n",
    "Удалим строки с пропущенными значениями целевой переменной. Мы учим модель находить зависимость между характеристиками дома и реальной ценой. Если мы заполняем пропуски медианой, мы подсовываем модели \"синтетику\". Модель будет учиться предсказывать не рыночную цену, а рассчитаюнную медиану. Если мы заполним пропуски в target и эти строки попадут в тестовую выборку (test), то при проверке качества мы будем проверять, насколько хорошо модель угадала нашу же медиану, а не реальность. В найденных на github решениях этой задачи - авторы заполняли пропуски в таргете медианой, что является ошибочным решением.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаляем строки, где в целевой переменной (target_clean) есть пропуски (NaN)\n",
    "df_estate = df_estate.dropna(subset=['target_clean'])\n",
    "\n",
    "# (Опционально) Сбрасываем индекс, чтобы он шел по порядку после удаления строк\n",
    "df_estate = df_estate.reset_index(drop=True)\n",
    "print(f'Количество пропусков целевой переменной = {df_estate[\"target_clean\"].isnull().sum()}.')\n",
    "print(f'Текущий размер датасета: {df_estate.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описательные статистики по целевой переменной(target_clean):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Основные статистики Price:\")\n",
    "print(df_estate['target_clean'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Медианная стоимость недвижимости $320 000 \n",
    "Средняя стоимость недвижимости $645 409 , что превышаем медианную стоимость в 2 раза.\n",
    "Кроме того, предположительно имеют место выбросы (минимальная стоимость $1, максимальная стоимость $195 000 000).\n",
    "В таком случае именно медианное значение цены будет лучше отражать меру центральной тенденции так как оно устойчиво к выбросам. \n",
    "\n",
    "Изучим распределение целевой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.histogram(\n",
    "    df_estate, \n",
    "    x='target_clean', \n",
    "    nbins=60, \n",
    "    marginal='rug', \n",
    "    title='Распределение стоимости недвижимости',\n",
    "    labels={'target_clean': 'Цена ($)'},\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig.update_layout(bargap=0.1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходная гистограмма цен крайне неинформативна: из-за экстремальных выбросов (до $2 \\times 10^8$) основное распределение \"схлопнулось\" в узкий пик около нуля. Чтобы избавиться от влияния этого длинного хвоста и сделать структуру данных видимой, мы применили логарифмическую трансформацию к целевой переменной\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['target_clean_log'] = np.log(df_estate['target_clean'])\n",
    "\n",
    "\n",
    "fig = px.histogram(\n",
    "    df_estate, \n",
    "    x='target_clean_log', \n",
    "    nbins=60, \n",
    "    # kde=True,  \n",
    "    marginal='box', \n",
    "    title='Распределение стоимости недвижимости (логарифмическая трансформация)',\n",
    "    labels={'target_clean_log': 'Log(Цена)'},\n",
    "    color_discrete_sequence=['#636EFA'],\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.update_layout(bargap=0.1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Проверка гипотезы о нормальности распределения цены (Log-transform) как обоснование использования target_clean_log для очистки выбросов\n",
    "\n",
    "Проверим, насколько распределение отличается от нормального. Используем D'Agostino's K^2 test. \n",
    "\n",
    "H0: Выборка имеет нормальное распределение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_orig, p_orig = normaltest(df_estate['target_clean'])\n",
    "stat_log, p_log = normaltest(np.log1p(df_estate['target_clean']))\n",
    "\n",
    "print(f\"Тест на нормальность распределения (D'Agostino's K^2 test):\")\n",
    "print(f\"   P-value (Исходная цена): {p_orig:.4e}\")\n",
    "print(f\"   P-value (Log-цена):      {p_log:.4e}\")\n",
    "\n",
    "# Чем меньше статистика, тем ближе к нормальному\n",
    "print(f\"Статистика (меньше = лучше): {stat_orig:.0f} -> {stat_log:.0f}\")\n",
    "print(\"Вывод: Логарифмирование существенно приближает распределение к нормальному,\")\n",
    "print(\"что критически важно для корректной работы моделей и удаления выбросов.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Избавимся от выбросов по целевому признаку, для этого воспользуемся методом Тьюки, применив его к логарифмированному признаку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_iqr(df, feature, left=1.5, right=1.5, log_scale=False):\n",
    "    \"\"\"\n",
    "    Находит выбросы в данных, используя метод межквартильного размаха. \n",
    "    Классический метод модифицирован путем добавления:\n",
    "    * возможности логарифмирования распредления\n",
    "    * ручного управления количеством межквартильных размахов в обе стороны распределения\n",
    "    Args:\n",
    "        df (pandas.DataFrame): набор данных\n",
    "        feature (str): имя признака, на основе которого происходит поиск выбросов\n",
    "        left (float, optional): количество межквартильных размахов в левую сторону распределения. По умолчанию 1.5.\n",
    "        right (float, optional): количество межквартильных размахов в правую сторону распределения. По умолчанию 1.5.\n",
    "        log_scale (bool, optional): режим логарифмирования. По умолчанию False - логарифмирование не применяется.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: наблюдения, попавшие в разряд выбросов\n",
    "        pandas.DataFrame: очищенные данные, из которых исключены выбросы\n",
    "    \"\"\"\n",
    "    if log_scale:\n",
    "        x = np.log(df[feature]+1)\n",
    "    else:\n",
    "        x= df[feature]\n",
    "    quartile_1, quartile_3 = x.quantile(0.25), x.quantile(0.75),\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * left)\n",
    "    upper_bound = quartile_3 + (iqr * right)\n",
    "    outliers = df[(x<lower_bound) | (x > upper_bound)]\n",
    "    cleaned = df[(x>lower_bound) & (x < upper_bound)]\n",
    "    return outliers, cleaned\n",
    "\n",
    "print(f'Тип ассиметрии до очистки: {df_estate[\"target_clean_log\"].skew()}')\n",
    "outliers, df_estate = find_outliers_iqr(df_estate, 'target_clean_log')\n",
    "print(f'Тип ассиметрии после очистки: {df_estate[\"target_clean_log\"].skew()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    df_estate, \n",
    "    x='target_clean_log', \n",
    "    nbins=60, \n",
    "    # kde=True,  \n",
    "    marginal='box', \n",
    "    title='Распределение логарифмической трансформации стоимости недвижимости (после очистки выбросов)',\n",
    "    labels={'target_clean_log': 'Log(Цена)'},\n",
    "    color_discrete_sequence=['#636EFA'],\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.update_layout(bargap=0.1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    df_estate, \n",
    "    x='target_clean', \n",
    "    nbins=60, \n",
    "    # kde=True,  <-- Удаляем эту строку\n",
    "    marginal='box', \n",
    "    title='Распределение стоимости недвижимости после логарифмической трансформации и очистки выбросов',\n",
    "    labels={'target_clean': 'Цена ($)'},\n",
    "    color_discrete_sequence=['#636EFA'],\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.update_layout(bargap=0.1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Число наблюдений {df_estate.shape[0]}.')\n",
    "df_estate['target_clean'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После очистки датасета количество наблюдений уменьшилось до 348064, аномальные значения в виде $1 или $195 млн. были удалены. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ неинформативных признаков\n",
    "\n",
    "Изучим потенциальные неинформативные признаки \n",
    "- наличие частного бассейна - private pool \n",
    "- наличие частного бассейна - PrivatePool, \n",
    "- наличие каминов - fireplace \n",
    "\n",
    "- в них порядка 65-85% пропусков и они кандидаты на удаление, \n",
    "\n",
    "признаки MlsId и mls-id, которые являются уникальными номерами в базе данных в системе множественного листинга и никакой информативности не несут.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_estate['private pool'].describe())\n",
    "display(df_estate['private pool'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_estate['PrivatePool'].describe())\n",
    "display(df_estate['PrivatePool'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_estate['fireplace'].describe())\n",
    "display(df_estate['fireplace'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_clear(df, no_inf_1, no_inf_2):\n",
    "    \"\"\" Удаляет столбцы, в которых доля пропущенных значений > 60%,\n",
    "    а также неинформативные признаки\n",
    "\n",
    "    Args:\n",
    "        df (DateFrame): исходный датафрейм\n",
    "        no_inf_1 (Series): первый неинформативный признак\n",
    "        no_inf_2 (Series): второй неинформативный признак\n",
    "\n",
    "    Returns:\n",
    "        DateFrame: очищенный датафрейм\n",
    "    \"\"\"\n",
    "    df = df.drop([no_inf_1, no_inf_2], axis=1)\n",
    "    #задаем минимальный порог: вычисляем 60% от числа строк\n",
    "    thresh = df.shape[0]*0.6\n",
    "    #удаляем столбцы, в которых более 30% (100-70) пропусков\n",
    "    df = df.dropna(thresh=thresh, axis=1)\n",
    "    return df\n",
    "\n",
    "df_estate = date_clear(df_estate, 'mls-id', 'MlsId')\n",
    "df_estate.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **status(статус продажи), propertyType(тип недвижимости), street(улица)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В признаках статус продажи(status), тип недвижимости(propertyType) и улица(street) заполним пропуски самым распространённым значением - for sale, single-family home и Address Not Disclosed соответственно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['status'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['status'] = df_estate['status'].fillna(df_estate['status'].describe().top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['propertyType'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['propertyType'] = df_estate['propertyType'].fillna(df_estate['propertyType'].describe().top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['street'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['street'] = df_estate['street'].fillna(df_estate['street'].describe().top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства работы переведём данные в признаках статус продажи(status), тип недвижимости(propertyType), улицу(street) в нижний регистр:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['status'] = df_estate['status'].apply(lambda x: x.lower())\n",
    "df_estate['propertyType'] = df_estate['propertyType'].apply(lambda x: x.lower())\n",
    "df_estate['street'] = df_estate['street'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вглянуть на статусы, то основная часть покрывается статусами:\n",
    "- for sale(продаётся)\n",
    "- active(активное)\n",
    "- foreclosure(обращение взыскания)\n",
    "- new construction()\n",
    "- pending(в ожидании)\n",
    "\n",
    "Оставим данные статусы, остальные переименуем в категорию other(другие)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['status'].value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# лист с категориями\n",
    "status_list = list(df_estate['status'].value_counts()[:5].index)\n",
    "df_estate['status'] = df_estate['status'].apply(lambda x: x if x in status_list else 'other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично посмотрим на тип недвижимости(propertyType):\n",
    "\n",
    "- single family(дом на одну семью)\n",
    "- condo(кондоминиум)\n",
    "- land(земля)\n",
    "- townhouse(таунхаус)\n",
    "- multi family(для многодетной семьи/большой семьи)\n",
    "- high rise(в выстоном здании)\n",
    "- ranch(ранчо)\n",
    "- coop(кооперативные квартиры/дома)\n",
    "- single detached(отдельно стоящий дом)\n",
    "- traditional(стандартное)\n",
    "\n",
    "Всё, что не попадает в вышеуказанные категории будут заменены на other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['propertyType'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проводим удаление(замену) ненужных символов \n",
    "df_estate['propertyType'] = df_estate['propertyType'].apply(lambda x: ' '.join(x.replace('-', ' ').replace(',', ' ').replace('/', ' ').split(' ')[0:2]))\n",
    "df_estate['propertyType'] = df_estate['propertyType'].replace('condo townhome', 'condo').replace('lot land', 'land').replace('detached ', 'single detached')\n",
    "# лист с категориями\n",
    "propertyType_list = list(df_estate['propertyType'].value_counts()[:11].index)\n",
    "# удаляем одну неинформационную категорию \n",
    "propertyType_list.remove('1 story')\n",
    "df_estate['propertyType'] = df_estate['propertyType'].apply(lambda x: x if x in propertyType_list else 'other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем признак улица(street), избавившись от номеров домов и оставив только наименование улиц - подразумевая, что стоимость недвижимость привязана к локации, а наименование улицы один из способов привязаться к этому местоположению(району города). Далее те улицы, которые будут в единственном экземпляре объединим в категорию other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['street'] = df_estate['street'].replace('undisclosed address', 'address not disclosed').replace('(undisclosed address)', 'address not disclosed')\\\n",
    "    .replace('address not available', 'address not disclosed').replace('unknown address', 'address not disclosed')\n",
    "    \n",
    "df_estate['street_new'] = df_estate['street'].apply(lambda x: ' '.join(x.split(' ')[1:4])).replace('', 'address not disclosed').replace('address not disclosed', 'not disclosed')\n",
    "\n",
    "df_estate = df_estate.drop(['street'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "street_list = list(df_estate['street_new'].value_counts()[df_estate['street_new'].value_counts()==1].index)\n",
    "date_street_one = df_estate[df_estate['street_new'].isin(street_list)]\n",
    "date_street_one['street_new'] = date_street_one['street_new'].apply(lambda x: 'other')\n",
    "df_estate = pd.concat(\n",
    "    [df_estate[~df_estate['street_new'].isin(street_list)], date_street_one],\n",
    "    ignore_index=True,\n",
    "    axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **city(город)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий признак - город(city), в котором имеются 20 пропущенных значений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['city'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### СТРАТЕГИЯ ЗАПОЛНЕНИЯ ПРОПУСКОВ ГЕОДАННЫХ:\n",
    "1. Если в исходнике нет города/штата/координат, берем их из pgeocode для надежности.\n",
    "2. Если и там нет - это мусорные данные\n",
    "\n",
    "ВАЖНО: Мы НЕ заполняем координаты средним значением. Локация - это ключевой драйвер цены. \"Средняя координата\" (центр США) для дома во Флориде создаст чудовищный шум в модели. Лучше удалить эти строки (их обычно < 1%), чем кормить модель ложью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. ЗАЩИТА ОТ ПОВТОРНОГО ЗАПУСКА (ОЧИСТКА)\n",
    "# Если мы перезапускаем ячейку, удаляем следы предыдущих попыток, чтобы merge не создавал дубликатов (_x, _y)\n",
    "temp_cols = ['zipcode_clean', 'city_geo', 'county_geo', 'state_geo', 'lat_geo', 'lon_geo']\n",
    "df_estate = df_estate.drop([c for c in temp_cols if c in df_estate.columns], axis=1)\n",
    "\n",
    "# 1. ПОДГОТОВКА И ОЧИСТКА ZIP-КОДОВ\n",
    "def clean_zip_code(x):\n",
    "    x = str(x).strip()\n",
    "    if x == '--' or x == '0' or pd.isna(x) or x == 'nan':\n",
    "        return None\n",
    "    return x.split('-')[0]\n",
    "\n",
    "df_estate['zipcode_clean'] = df_estate['zipcode'].apply(clean_zip_code)\n",
    "\n",
    "# 2. ГЕНЕРАЦИЯ ГЕОДАННЫХ (pgeocode)\n",
    "print(\"Запрос актуальных геоданных через pgeocode...\")\n",
    "nomi = pgeocode.Nominatim('us')\n",
    "\n",
    "unique_zips = df_estate['zipcode_clean'].dropna().unique().astype(str).tolist()\n",
    "\n",
    "# Запрос данных\n",
    "geo_info = nomi.query_postal_code(unique_zips)\n",
    "\n",
    "# Отбираем нужные колонки\n",
    "geo_ref = geo_info[['postal_code', 'place_name', 'county_name', 'state_code', 'latitude', 'longitude']].copy()\n",
    "geo_ref.columns = ['zipcode_clean', 'city_geo', 'county_geo', 'state_geo', 'lat_geo', 'lon_geo']\n",
    "\n",
    "# 3. ОБЪЕДИНЕНИЕ И УТОЧНЕНИЕ ДАННЫХ\n",
    "df_estate = df_estate.merge(geo_ref, on='zipcode_clean', how='left')\n",
    "\n",
    "# СТРАТЕГИЯ ЗАПОЛНЕНИЯ:\n",
    "mappings = [\n",
    "    ('latitude', 'lat_geo'), \n",
    "    ('longitude', 'lon_geo'), \n",
    "    ('city', 'city_geo'), \n",
    "    ('state', 'state_geo'), \n",
    "    ('county', 'county_geo')\n",
    "]\n",
    "\n",
    "for col, geo_col in mappings:\n",
    "    if col in df_estate.columns:\n",
    "        # Если колонка уже была (например, city), заполняем только пропуски\n",
    "        df_estate[col] = df_estate[col].fillna(df_estate[geo_col])\n",
    "    else:\n",
    "        # Если колонки не было (latitude), создаем её из гео-данных\n",
    "        df_estate[col] = df_estate[geo_col]\n",
    "\n",
    "# Удаляем вспомогательные колонки\n",
    "df_estate = df_estate.drop(temp_cols, axis=1, errors='ignore')\n",
    "\n",
    "# 4. ОБРАБОТКА ОСТАВШИХСЯ ПРОПУСКОВ\n",
    "missing_coords = df_estate['latitude'].isnull().sum()\n",
    "print(f\"Строк без координат после обогащения данными: {missing_coords}\")\n",
    "\n",
    "if missing_coords > 0:\n",
    "    print(f\"Удаляем {missing_coords} строк с неизвестной локацией...\")\n",
    "    df_estate = df_estate.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "# Сбрасываем индекс для порядка\n",
    "df_estate = df_estate.reset_index(drop=True)\n",
    "\n",
    "print(f\"Итоговый размер датасета: {df_estate.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **baths(число ванных комнат)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Займёмся признаком число ваных(baths) - данный признак содержит разные строковые значения и символы, которые требуют обработки, также имеется в числовых значениях(представленых в виде строкового) имеется символ ',' вместо точки, его необходимо заменить на точку для корректного преобразования в числовой формат(float). Пропущенные значения заменим на медианное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler_baths(df):\n",
    "    \"\"\" Обработчик признака baths\n",
    "\n",
    "    Args:\n",
    "        df (DateFrame): входящий датафрейм\n",
    "\n",
    "    Returns:\n",
    "        DateFrame: обработанный датафрейм\n",
    "    \"\"\"\n",
    "    # проверяем на пропущенное значение\n",
    "    if df  is not np.nan:\n",
    "        # производим замену симвволов, производим сплит, преобразуем в лист\n",
    "        df = df.replace(',', '.').split(' ')\n",
    "        # проходим по каждому элементу листа\n",
    "        for i in df:\n",
    "            try:\n",
    "                i = float(i)\n",
    "                return i\n",
    "            # обрабатываем исключение - если элемент невозможно преобразовать в float, то пропускаем\n",
    "            except ValueError:\n",
    "                continue  \n",
    "    else:\n",
    "        return df\n",
    "    \n",
    "df_estate['baths'] = df_estate['baths'].apply(handler_baths)\n",
    "df_estate['baths'] = df_estate['baths'].fillna(df_estate['baths'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После преобразования имеем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['baths'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу смущает максимальные значения в 750, скорее всего это выбросы, которые будут сбивать модель, избавимся от них ограничив размер 20. Нулевые значения оставляем, так как в датафрейме имеются тип недвижимости земля(land), что вполне объясняет нулевые значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate = df_estate[df_estate['baths'] < 21].reset_index().drop(['index'], axis=1)\n",
    "df_estate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **sqft(квадратный фунт)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмёмся за признак **квадратный фут(sqft)** - напишем функцию-обработчик, обратим внимание, что в значениях имеются записи в формате: '--', '2500 sqft', '-- sqft', 'Total interior livable area: 2,533 sqft', а также пропущенные значения, которые заменим на медианное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['sqft'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler_sqft(val):\n",
    "    \"\"\" Обрабатывает признак sqft \"\"\"\n",
    "    \n",
    "    # 1. Проверка на NaN (используем pd.isna для надежности) или явные пропуски\n",
    "    if pd.isna(val) or val == '--':\n",
    "        return np.nan\n",
    "    \n",
    "    # Превращаем в строку и убираем запятые сразу для всех проверок\n",
    "    val = str(val).replace(',', '').strip()\n",
    "    \n",
    "    # 2. Проверка на мусорные строки, начинающиеся с тире\n",
    "    if val.startswith('--'):\n",
    "        return np.nan\n",
    "    \n",
    "    # 3. ОБРАБОТКА ДИАПАЗОНА (исправление для '610-840')\n",
    "    if '-' in val:\n",
    "        try:\n",
    "            # Разбиваем '610-840' на ['610', '840'], переводим в числа и считаем среднее\n",
    "            parts = [float(x) for x in val.split('-')]\n",
    "            return sum(parts) / len(parts)\n",
    "        except ValueError:\n",
    "            pass # Если не получилось, идем дальше\n",
    "            \n",
    "    # 4. Обработка формата 'Total interior livable area: ...'\n",
    "    if 'total' in val.lower():\n",
    "        try:\n",
    "            # Берем предпоследний элемент\n",
    "            return float(val.split()[-2])\n",
    "        except (ValueError, IndexError):\n",
    "            return np.nan\n",
    "\n",
    "    # 5. Стандартный случай ('1200 sqft' -> берем первое слово)\n",
    "    try:\n",
    "        return float(val.split(' ')[0])\n",
    "    except ValueError:\n",
    "        # Если ничего не помогло (например, совсем битая строка), возвращаем NaN\n",
    "        return np.nan\n",
    "\n",
    "# Применяем функцию\n",
    "df_estate['sqft_new'] = df_estate['sqft'].apply(handler_sqft)\n",
    "\n",
    "# Заполняем пропуски медианой\n",
    "df_estate['sqft_new'] = df_estate['sqft_new'].fillna(df_estate['sqft_new'].median())\n",
    "\n",
    "# Удаляем старый столбец\n",
    "df_estate = df_estate.drop(['sqft'], axis=1)\n",
    "\n",
    "# Проверяем результат\n",
    "df_estate['sqft_new'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **stories(число этажей)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий признак - **число этажей(stories)** - в нём данные представлены в строковом формате, некоторые признаки прописаны словами, так, например, первый этаж может быть прописан 'One', 'One,' '1 Story', '1.0', есть пропущенные значения, спец. символы, например, '+', записи не содержащие никакие отсылки к этажности(неинформативные) - их заменим на медианное значение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['stories'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler_stories(df):\n",
    "    \"\"\" Обработчик признака stories\n",
    "\n",
    "    Args:\n",
    "        df (Series): необработанный признак\n",
    "\n",
    "    Returns:\n",
    "        float/np.nan: обработанный признак\n",
    "    \"\"\"\n",
    "    # словарь для замены номеров этажей\n",
    "    stories_dict = {'One':1, 'Two':2, 'Three':3, 'Tri-Level':3, 'One,':1, '1-2':1.5, 'Bi-Level':2, '3-4':3.5, 'Two,':2, 'Tri':3}\n",
    "    \n",
    "    if df is not np.nan:\n",
    "        df = df.replace('+', '').split(' ')[0]\n",
    "        if df in stories_dict.keys():\n",
    "            df = stories_dict[df]\n",
    "            try:\n",
    "                df = float(df)\n",
    "                return df\n",
    "            except ValueError:\n",
    "                df = np.nan\n",
    "                return df\n",
    "        else:\n",
    "            try:\n",
    "                df = float(df)\n",
    "                return df\n",
    "            except ValueError:\n",
    "                df = np.nan\n",
    "                return df\n",
    "    else:\n",
    "        return df\n",
    "    \n",
    "df_estate['stories_new'] = df_estate['stories'].apply(handler_stories)\n",
    "df_estate = df_estate.drop(['stories'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратим внимание на два объекта с этажностью значением более 100, явно это выбросы от которых необходимо избавиться:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate[df_estate['stories_new'] > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['stories_new'] = df_estate['stories_new'].fillna(df_estate['stories_new'].median())\n",
    "df_estate = df_estate[df_estate['stories_new'] < 100].reset_index().drop(['index'], axis=1)\n",
    "df_estate['stories_new'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **beds(число комнат)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler_beds(df):\n",
    "    \"\"\" Обработчик признака beds\n",
    "\n",
    "    Args:\n",
    "        df (Series): необработанный признак\n",
    "\n",
    "    Returns:\n",
    "        float/np.nan: обработанный признак\n",
    "        \"\"\"\n",
    "    beds_list = list(range(0,16))\n",
    "    if df is not np.nan:\n",
    "        try:\n",
    "            df = float(df.replace(',', '.').split(' ')[0])\n",
    "            if df in beds_list:\n",
    "                return df\n",
    "            else:\n",
    "                return np.nan  \n",
    "        except ValueError:\n",
    "            return np.nan    \n",
    "    else:\n",
    "        return df\n",
    "    \n",
    "df_estate['beds_new'] = df_estate['beds'].apply(handler_beds)      \n",
    "df_estate = df_estate.drop(['beds'], axis=1)\n",
    "df_estate['beds_new'] = df_estate['beds_new'].fillna(df_estate['beds_new'].median())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Сведения о доме(homeFacts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Займёмся признаком сведения о доме(homeFacts) - в котором намешано много информации и она представлена в строковом формате, который в свою очередь похож на json-формат(данные хранятся в виде словарей и списков). Внимательно взглянем на информацию, содержащуюся в данном признаке:\n",
    "\n",
    "- atAGlanceFacts(факты) - основной ключ, значениями которого является список словарей\n",
    "\n",
    "В каждом словаре находятся два ключа -**factLabel**(наименование параметра/факта) и **factValue**(значение параметра). \n",
    "\n",
    "Имеем следующие виды фактов(factValue):\n",
    "\n",
    "- Year built(год постройки дома)\n",
    "- Remodeled year(год реконструкции)\n",
    "- Heating(отопление)\n",
    "- Cooling(охлаждение)\n",
    "- Parking(паркинг)\n",
    "- lotsize(дом с участком)\n",
    "- Price/sqft(цена за квадратный фунт)\n",
    "\n",
    "Задача - разнести все параметры по отдельным признакам, далее преобразовать их, избавиться от выбросов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# преобразование строки в словарь\n",
    "df_estate['homeFacts_new'] = df_estate['homeFacts'].apply(lambda x: ast.literal_eval(x))\n",
    "# создание новых признаков\n",
    "df_estate['year_built'] = df_estate['homeFacts_new'].apply(lambda x: x['atAGlanceFacts'][0]['factValue'])\n",
    "df_estate['remodeled_year'] = df_estate['homeFacts_new'].apply(lambda x: x['atAGlanceFacts'][1]['factValue'])\n",
    "df_estate['heating'] = df_estate['homeFacts_new'].apply(lambda x: x['atAGlanceFacts'][2]['factValue'])\n",
    "df_estate['cooling'] = df_estate['homeFacts_new'].apply(lambda x: x['atAGlanceFacts'][3]['factValue'])\n",
    "df_estate['parking'] = df_estate['homeFacts_new'].apply(lambda x: x['atAGlanceFacts'][4]['factValue'])\n",
    "df_estate['lotsize'] = df_estate['homeFacts_new'].apply(lambda x: x['atAGlanceFacts'][5]['factValue'])\n",
    "df_estate['price_sqft'] = df_estate['homeFacts_new'].apply(lambda x: x['atAGlanceFacts'][6]['factValue'])\n",
    "# удаление первначального признака\n",
    "df_estate = df_estate.drop(['homeFacts'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **year_built(год постройки дома)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взглянем на year_built:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['year_built'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['year_built'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "имеем 3092(+44584) незаполненных значений. Преобразуем признак в числовой формат и заполним пропуски медианным значением. Также заменим на медианные значения дома года постройки > 2025 года и раньше 1700 года."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['year_built'] = df_estate['year_built'].apply(lambda x: np.nan if x == '' else np.nan if x == 'No Data' else np.nan if x is None else x)\n",
    "df_estate['year_built'] = df_estate['year_built'].apply(lambda x: int(x) if x is not np.nan else x)\n",
    "df_estate['year_built'] = df_estate['year_built'].fillna(df_estate['year_built'].median())\n",
    "df_estate['year_built'] = df_estate['year_built'].apply(lambda x: df_estate['year_built'].median() if x > 2025 else df_estate['year_built'].median() if x < 1700 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['year_built'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо года постройки создадим новый признак - возраст дома(age_home) - разницой текущего года и года постройки дома."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# текущий год\n",
    "year_current = datetime.datetime.now().year\n",
    "df_estate['age'] = year_current - df_estate['year_built']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **remodeled_year(год реконструции)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['remodeled_year'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['remodeled_year'] = df_estate['remodeled_year'].apply(lambda x: np.nan if (x == '') | (x == None) else x)\n",
    "df_estate['remodeled_year'] = df_estate['remodeled_year'].apply(lambda x: int(x) if x is not np.nan else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['remodeled_year'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В признаке 24898 пропуска, заменим их на медианное значение - 1986:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['remodeled_year'] = df_estate['remodeled_year'].fillna(df_estate['remodeled_year'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу стоит проверить - дата реконструции не может быть раньше(либо равно) даты постройки, поэтому все значения в признаке remodeled_year, которые не будут соответствовать этому условию заменим на нули."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['remodeled_year'] = df_estate.apply(lambda x: 0 if x.remodeled_year <= x.year_built else x.remodeled_year, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также создадим признак на основе remodeled_year - число лет, прошедших с реконструкции(age_remodeled). Обратим внимание, что если в признаке remodeled_year имеется 0, то тогда время реставрации считается с даты постройки дома:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['age_remodeled'] = df_estate.apply(lambda x: (year_current - x.remodeled_year) if x.remodeled_year != 0 else (year_current - x.year_built), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После этого можем удалить признаки year_built и remodeled_year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate = df_estate.drop(['year_built', 'remodeled_year'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **heating(отопление)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['heating'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В признаке 93105 пропущенных значений, так как значения текстовые, то приведём их для удобства в нижний регистр:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['heating'] = df_estate['heating'].apply(lambda x: x.lower() if (type(x) is not float) and (x is not None) else x)\n",
    "df_estate['heating'] = df_estate['heating'].apply(lambda x: np.nan if (x is None) | (x == 'none') | (x == '') else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Избавимся от ненужных символов, возьмём только первые два элемента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['heating'] = df_estate['heating'].apply(lambda x: x.replace(',', '') if type(x) is not float else x)\n",
    "df_estate['heating'] = df_estate['heating'].apply(lambda x: x if (x is np.nan) | (type(x) is float) else ' '.join(x.split(' ')[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вглянем на первые 50 самых распространённых значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['heating'].value_counts()[:51]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Унифицируем данные, произведя необходимые замены, далее отберём первые 9 популярных значений, а остальные заменим на other. Пропущенные значения заменим самой распространённой категорией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['heating'] = df_estate['heating'].replace('central electric', 'electric').replace(' electric', 'electric').replace('no data', np.nan).replace('electric heat', 'electric')\\\n",
    "    .replace('natural gas', 'gas').replace(' gas', 'gas').replace('central gas', 'gas').replace('gas heat', 'gas').replace('electric gas', 'gas').replace('gas hot', 'gas')\\\n",
    "        .replace('gas -', 'gas').replace('propane', 'gas').replace(' heat', 'heat pump').replace('heating system', 'central').replace('central furnace', 'central')\\\n",
    "            .replace('central heating', 'central').replace('central heat', 'central').replace('heat pump(s)', 'heat pump').replace('radiant', 'central').replace('baseboard forced', 'baseboard')\\\n",
    "                .replace('baseboard hot', 'baseboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# лист из топ 9 категорий\n",
    "heating_list = list(df_estate['heating'].value_counts()[:9].index)\n",
    "\n",
    "df_estate['heating'] = df_estate['heating'].apply(lambda x: x if x is np.nan else x if x in heating_list else 'other')\n",
    "df_estate['heating'] = df_estate['heating'].fillna(df_estate['heating'].describe().top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого получаем уменьшение категорий до 9:\n",
    "\n",
    "- forced air(воздушное принудительное отопление)\n",
    "- other(прочее)\n",
    "- electric(электрическое)\n",
    "- gas(газовое)\n",
    "- central(центральное)\n",
    "- central air(централье воздушное)\n",
    "- heat pump(тепловые насосы)\n",
    "- baseboard(тёплый пол)\n",
    "- wall(тепловая стена)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['heating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **cooling(охлаждение)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В признаке типа охлаждения имеем 3092 пропуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['cooling'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['cooling'] = df_estate['cooling'].apply(lambda x: x.lower() if (x is not np.nan) & (x is not None) else x)\n",
    "df_estate['cooling'] = df_estate['cooling'].apply(lambda x: np.nan if (x is None) | (x == 'none') | (x == '') else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['cooling'].value_counts()[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Произведём преобразование записей и разделим на категории:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['cooling'] = df_estate['cooling'].replace('no data', np.nan).replace('none', np.nan)\n",
    "df_estate['cooling'] = df_estate['cooling'].apply(lambda x: x if x is np.nan else ' '.join(x.split(' ')[:2]))\n",
    "df_estate['cooling'] = df_estate['cooling'].apply(lambda x: x.replace(',', '') if type(x) is not float else x)\n",
    "df_estate['cooling'] = df_estate['cooling'].replace('has cooling', 'cooling').replace('central a/c', 'central air')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все значения, которые не войдут в топ-10 категорий обозначим как other, а пропущенные значение заменим самым распространённым значением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# топ лист\n",
    "cooling_list = list(df_estate['cooling'].value_counts()[:10].index)\n",
    "df_estate['cooling'] = df_estate['cooling'].apply(lambda x: x if x is np.nan else x if x in cooling_list else 'other')\n",
    "df_estate['cooling'] = df_estate['cooling'].fillna(df_estate['cooling'].describe().top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого в cooling(охлаждение) имеем категории:\n",
    "\n",
    "- central(центральное)\n",
    "- central air(центральное воздушное)\n",
    "- other(прочее)\n",
    "- cooling(охлаждение)\n",
    "- central electric(центральное электрическое)\n",
    "- central gas(центральное газ)\n",
    "- wall(настенный)\n",
    "- central heating(центральное отопление/охлаждение)\n",
    "- cooling system(системы охлаждения)\n",
    "- refrigeration(охлаждение)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['cooling'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **parking(паркинг)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В признаке 3092(+150940) пропусков и очень много уникальных записей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['parking'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['parking'].value_counts()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Произведём преобразование признака и унификацию категорий, далее оставим топ-15, остальное обозначим как other, пропуски заполним топовым значением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['parking'] = df_estate['parking'].apply(lambda x: x.lower() if (x is not np.nan) & (x is not None) else x)\n",
    "df_estate['parking'] = df_estate['parking'].apply(lambda x: np.nan if (x is None) | (x == '') else x)\n",
    "df_estate['parking'] = df_estate['parking'].replace('no data', np.nan).replace('none', np.nan)\n",
    "df_estate['parking'] = df_estate['parking'].apply(lambda x: x if x is np.nan else x.replace('+', '').replace(',', '').split(' ')[0])\n",
    "df_estate['parking'] = df_estate['parking'].replace('driveway', 'off').replace('garage', 'detached')\n",
    "# топ лист\n",
    "parking_list = list(df_estate['parking'].value_counts()[:15].index)\n",
    "\n",
    "df_estate['parking'] = df_estate['parking'].apply(lambda x: x if x is np.nan else x if x in parking_list else 'other')\n",
    "df_estate['parking'] = df_estate['parking'].fillna(df_estate['parking'].describe().top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В признаке parking(паркинг) осталось 16 категорий:\n",
    "\n",
    "- attached(пристроенный гараж)\n",
    "- 0/1/2/3/4/5/6(количество мест)\n",
    "- detached(отдельный гараж)\n",
    "- carport(навес)\n",
    "- off(место вне улицы)\n",
    "- other(прочие)\n",
    "- assigned(закреплённое место)\n",
    "- parking(стоянка)\n",
    "- electric(с электрическим приводом/воротами)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['parking'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **lotsize(дом с участком)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['lotsize'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['lotsize'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В признаке 26667(+30552)пропущенных значений, но на самом деле их больше так как имеются неявные пропуски в виде \"—\", \"No Data\", \"-- sqft lot\". Видим, что значения занесены в разных единицах измерения - акры(acres) и квадратные фунты(sqft) - переведём всё к одному виду - sqft. Обратим внимание, что где-то имеется разделитель в виде точки, а где-то в виде запятой. После обработки заменим пропущенные значения медианным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acr_change(acr):\n",
    "    \"\"\" Переводит акры в квадратные фунты\n",
    "\n",
    "    Args:\n",
    "        acr (str): значение в акрах\n",
    "\n",
    "    Returns:\n",
    "        float: значение в квадратных фунтах\n",
    "    \"\"\"\n",
    "    sqft = 43560 * float(acr)\n",
    "    return sqft\n",
    "\n",
    "def handler_lotsize(df):\n",
    "    \"\"\" Обработчик признака lotsize\n",
    "\n",
    "    Args:\n",
    "        df (Series): необработанный признак\n",
    "\n",
    "    Returns:\n",
    "        float/str: обработанный признак\n",
    "    \"\"\"\n",
    "    \n",
    "    # заменяем пропуски на No и приводим разделитель к общему виду - точки\n",
    "    df = df.replace('—', 'No').replace('No Data', 'No').replace('-- sqft lot', 'No').replace(',', '')\n",
    "    \n",
    "    if df != 'No':\n",
    "        # приводим строку к нижнему регистру\n",
    "        df = df.lower()\n",
    "        # создаём лист со значениями \n",
    "        list_temp = df.split(' ')\n",
    "        # если строка состоит из более чем одного значения, и второе значение 'acres' или 'acre', то переводим в кв фунты\n",
    "        if (len(list_temp) > 1) and (df.split(' ')[1] == 'acres' or df.split(' ')[1] == 'acre'):\n",
    "            df = acr_change(list_temp[0])\n",
    "            return df\n",
    "        else:\n",
    "            df = float(list_temp[0])\n",
    "            return df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# меняем все пропущенные значения для удобства на No\n",
    "df_estate['lotsize'] = df_estate['lotsize'].apply(lambda x: 'No' if (x is np.nan) | (x is None) | (x == '') else x)  \n",
    "# применяем функцию обработчик\n",
    "df_estate['lotsize'] = df_estate['lotsize'].apply(handler_lotsize)\n",
    "# все No меняем на np.nan, отрицательные значения на 0\n",
    "df_estate['lotsize'] = df_estate['lotsize'].apply(lambda x: np.nan if x == 'No' else 0 if x < 0 else x)\n",
    "# пропуски заменяем на медианное значение\n",
    "df_estate['lotsize'] = df_estate['lotsize'].fillna(df_estate['lotsize'].describe().median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **price_sqft(цена за квадратный фунт)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['price_sqft'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['price_sqft'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В признаке имеется явный 41964 (+4304) пропуск значений и не явный в виде записей 'No Data', 'No Info', которые заменим на np.nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['price_sqft'] = df_estate['price_sqft'].apply(lambda x: np.nan if (x == 'No Data') | (x == 'No Info') | (x is None) | (x == '') else x)\n",
    "df_estate['price_sqft'] = df_estate['price_sqft'].apply(lambda x: x if x is np.nan else float(x.replace('$', '').replace('/', ' ').replace(',', '').split(' ')[0]))\n",
    "# пропуски заменяем на медианное значение\n",
    "df_estate['price_sqft'] = df_estate['price_sqft'].fillna(df_estate['price_sqft'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как разобрались с блоком **homeFacts_new(сведения о доме)** данный признак можно удалить:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate = df_estate.drop(['homeFacts_new'], axis=1)\n",
    "df_estate = df_estate.rename(columns={'street_new':'street', 'sqft_new':'sqft', 'stories_new':'stories', 'beds_new':'beds'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **schools(школы)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признак schools(школы) по структуре похож на признак honeFacts(сведения о доме) - формат json файла с вложенными словарями и списками в списке содержатся следующие данные:\n",
    "\n",
    "- rating(рейтинг школ)\n",
    "- data, содержащий несколько сведений:\n",
    "    - Distance(дистанции до школ, в милях)\n",
    "    - Grades(уровень школы - младшая/средняя/старшая/и тд)\n",
    "    - name(наименование школы)\n",
    "\n",
    "Нас будет интересовать только rating(рейтинг школ) и Distance(дистанции до школ, в милях), остальные признаки упустим. Вначале преобразуем строковое значение в словварь и вытащим необходимые данные, на основе которых создадим два новых признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate.iloc[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['rating'] = df_estate['schools'].apply(lambda x: ast.literal_eval(x)[0]['rating'])\n",
    "df_estate['distance'] = df_estate['schools'].apply(lambda x: ast.literal_eval(x)[0]['data']['Distance'])\n",
    "#df_estate['grades'] = df_estate['schools'].apply(lambda x: ast.literal_eval(x)[0]['data']['Grades'])\n",
    "df_estate = df_estate.drop(['schools'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **rating(рейтинг школ)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждое значение признака rating(рейтинг школ) представлен в виде списка с рейтингами школ,, список может быть пустым либо содержать 'NR'(нет рейтинга), 'NA'(нет данных). Мы создадим обобщающий признак - среднее значение рейтинга школ в данном районе - rating_mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['rating'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler_rating(df):\n",
    "    \"\"\" Обработчик признака rating\n",
    "\n",
    "    Args:\n",
    "        df (Series): необработанный признак\n",
    "\n",
    "    Returns:\n",
    "        float/np.nan: обработанный признак\n",
    "    \"\"\"\n",
    "    temp_list = []\n",
    "    list_rating = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "    \n",
    "    if df == []:\n",
    "        return np.nan\n",
    "    else:\n",
    "        for i in df:\n",
    "            i = i.split('/')[0]\n",
    "            if i in list_rating:\n",
    "                temp_list.append(float(i))\n",
    "        rating_mean = round(np.mean(temp_list), 2)\n",
    "        return rating_mean   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['rating_mean'] = df_estate['rating'].apply(handler_rating)\n",
    "df_estate['rating_mean'] = df_estate['rating_mean'].fillna(round(df_estate['rating_mean'].mean(), 2))\n",
    "df_estate = df_estate.drop(['rating'], axis=1)                                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **disance(растояние до школ)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном признаке, представленном в виде списка из расстояний до школ мы создадим два признака - среднее расстояние до школы(distance_mean) и количество школ на районе(schools_count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['distance'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler_distance(df):\n",
    "    \"\"\" Обработчик признака distance\n",
    "\n",
    "    Args:\n",
    "        df (Serise): необработанный признак\n",
    "\n",
    "    Returns:\n",
    "        list: обработанные признаки - средняя дистанция и количество школ\n",
    "    \"\"\"\n",
    "    temp_list = []\n",
    "    \n",
    "    for i in df:\n",
    "        i = i.split('mi')[0]\n",
    "        temp_list.append(float(i))\n",
    "    dist_mean = round(np.mean(temp_list), 2)\n",
    "    schools_count = len(temp_list)\n",
    "    return [dist_mean, schools_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['distance_mean'] = df_estate['distance'].apply(lambda x: handler_distance(x)[0])\n",
    "df_estate['schools_count'] = df_estate['distance'].apply(lambda x: handler_distance(x)[1])\n",
    "df_estate['schools_count'] = df_estate['schools_count'].apply(lambda x: df_estate['schools_count'].median() if x==0 else x)\n",
    "df_estate['distance_mean'] = df_estate['distance_mean'].fillna(round(df_estate['distance_mean'].mean(), 2))                                                 \n",
    "df_estate = df_estate.drop(['distance'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак в очищенном датасете осталось 346004 наблюдения, взглянем ещё раз на целевую переменную, а далее попытаемся найти зависимости в признаках и сформировать гипотезы/выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=df_estate,\n",
    "    x='target_clean',    \n",
    "    title='Distribution of the target_clean',\n",
    "    text_auto=True,    \n",
    "    height=800,    \n",
    "    width=1500,\n",
    "    marginal='box'     \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ассиметрия \n",
    "print(df_estate['target_clean'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_estate['target_clean'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распределение целевой переменной стало иметь визуально читаемый вид, но признак ассиметричный - коэффициент ассиметрии положительный(2.55), что говорит о смещении вправо - на графике имеем длинный правый хвост - в котором имеются наблюдения с большим диапазоном разброса цен:\n",
    "\n",
    "- Медианное значение увеличилось до $328162\n",
    "- Среднее - $484201\n",
    "- Минимальное - $34250\n",
    "- Максимальное - $3202966"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **status - target_clean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=df_estate.groupby(['status'], as_index=False)['target_clean'].count(),\n",
    "    x='status',\n",
    "    y= ['target_clean'],\n",
    "    color='status',\n",
    "    height=500, \n",
    "    width=1000, \n",
    "    title='Distribution of the attribute status',\n",
    "    text_auto=True   \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    data_frame= df_estate.groupby(['status'], as_index=False)['target_clean'].median(),\n",
    "    x='status',\n",
    "    y='target_clean',\n",
    "    color='status',\n",
    "    text='target_clean', \n",
    "    orientation='v',\n",
    "    title='Median value of the price from the status',\n",
    "    height=500, \n",
    "    width=1000   \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    data_frame= df_estate.groupby(['status'], as_index=False)['distance_mean'].median(),\n",
    "    x='status',\n",
    "    y='distance_mean',\n",
    "    color='status',\n",
    "    text='distance_mean', \n",
    "    orientation='v',\n",
    "    title='Average distance to school by status',\n",
    "    height=500, \n",
    "    width=1000   \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данных основная масса объявлений это со статусом **for sale**(224224), наименьшая группа **pending**(4378). \n",
    "\n",
    "Медианная стоимость недвижимости в зависимости от статуса разнится - наибольшая зафиксирована в категории **new construction**(449950), что вполне объяснимо - новое, более современное жильё и строится в более перспективных районах города. Стоит отметить и категории **foreclosure** - недвижимость должников для продажи ниже рынка(203286) и категории **pending** - возможно недвижимость в непристижных районах/либо далеко от инфраструктцры/школ - на графике видно, что среднее расстояние до школы больше(1,99 мили)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Статистический анализ данных\n",
    "\n",
    "Ниже преведены для подтверждения статистические тесты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_sale = df_estate[df_estate['status']=='for sale']['target_clean'].values\n",
    "active = df_estate[df_estate['status']=='active']['target_clean'].values\n",
    "foreclosure = df_estate[df_estate['status']=='foreclosure']['target_clean'].values\n",
    "new_construction = df_estate[df_estate['status']=='new construction']['target_clean'].values\n",
    "pending = df_estate[df_estate['status']=='pending']['target_clean'].values\n",
    "other = df_estate[df_estate['status']=='other']['target_clean'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform one-way ANOVA test\n",
    "_, p = f_oneway(for_sale, active, foreclosure, new_construction, pending, other)\n",
    "\n",
    "H0 = 'Нет значимой разницы между ценой недвижимости с разными статусами продажи'\n",
    "H1 = 'Есть значимая разница между ценой недвижимости с разными статусами продажи'\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "if p > alpha:\n",
    "    print(f'{p} > {alpha} мы не можем отвергнуть нулевую гипотезу. {H0}')\n",
    "else:\n",
    "    print(f'{p} <= {alpha} мы отвергаем нулевую гипотезу. {H1}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform Kruskal-Wallis Test \n",
    "_, p = stats.kruskal(for_sale, active, foreclosure, new_construction, pending, other)\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "if p > alpha:\n",
    "    print(f'{p} > {alpha} мы не можем отвергнуть нулевую гипотезу. {H0}')\n",
    "else:\n",
    "    print(f'{p} <= {alpha} мы отвергаем нулевую гипотезу. {H1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  непараметрический post-hoc тест, который сравнивает каждую группу с каждой после Kruskal–Wallis\n",
    "data = [for_sale, active, foreclosure, new_construction, pending, other]\n",
    "sp.posthoc_dunn (data, p_adjust = 'bonferroni')[sp.posthoc_dunn (data, p_adjust = 'bonferroni') < 0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проанализируем матрицу p-value попарных сравнений между 6 группами (for_sale, active, foreclosure, new_construction, pending, other), где \n",
    "NaN — сравнение группы с самой собой, а все остальные значения — p-value после коррекции Бонферрони. \n",
    "\n",
    "Во всех ячейках p-value меньше 0.05 (на самом деле — намного меньше, вплоть до 10⁻²⁹³).\n",
    "Нет ни одного значения, близкого к порогу значимости.\n",
    "Такие экстремально маленькие p-value означают, что различия между распределениями групп очень сильные.\n",
    "\n",
    "Каждая категория имеет своё собственное распределение цен, и оно существенно отличается от всех остальных категорий.\n",
    "\n",
    "Главный вывод\n",
    "**Все группы статистически значимо отличаются друг от друга.** Каждая категория имеет своё собственное распределение цен,\n",
    "и оно существенно отличается от всех остальных категорий.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **propertyType - target_clean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=df_estate.groupby(['propertyType'], as_index=False)['target_clean'].count().sort_values(by=['target_clean'], ascending=False),\n",
    "    x='propertyType',\n",
    "    y= ['target_clean'],\n",
    "    color='propertyType',\n",
    "    height=500, \n",
    "    width=1000, \n",
    "    title='Distribution of the attribute propertyType',\n",
    "    text_auto=True   \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    data_frame= df_estate.groupby(['propertyType'], as_index=False)['target_clean'].median().sort_values(by=['target_clean'], ascending=False),\n",
    "    x='propertyType',\n",
    "    y='target_clean',\n",
    "    color='propertyType',\n",
    "    text='target_clean', \n",
    "    orientation='v',\n",
    "    title='Median value of the price from the propertyType',\n",
    "    height=500, \n",
    "    width=1000   \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более 2/3 объявлений о продаже недвижимости приходится на категорию **single family**(на одну семью), также видим, что данный признак существенно влияет на цену недвижимости, так для земельных участков(**land**) она наименьшая - $150000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **state - target_clean**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим на распределение объявлений в штатах и зависимость целевой переменной от данного признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=df_estate.groupby(['state'], as_index=False)['target_clean'].count().sort_values(by=['target_clean'], ascending=False),\n",
    "    x='state',\n",
    "    y= ['target_clean'],\n",
    "    color='state',\n",
    "    height=500, \n",
    "    width=1000, \n",
    "    title='Distribution of the attribute propertyType',\n",
    "    text_auto=True   \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отобразим объекты на географической карте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_int_size(value):\n",
    "    try:\n",
    "        return np.log10(int(value))\n",
    "    except:\n",
    "        return np.log10(int(value.split('[')[0]))\n",
    "\n",
    "fig = go.Figure(go.Scattermapbox(lat=df_estate['latitude'], lon=df_estate['longitude'], text=df_estate[['city', 'age']], name='Estate on a geographical map', \n",
    "                                 marker=dict(size=df_estate['target_clean'].map(to_int_size), colorbar=dict(title=\"Age\"), color=df_estate['age'])))\n",
    "# центрирование карты на Нью-Йорк\n",
    "capital = df_estate[df_estate['city']=='New York']\n",
    "map_center = go.layout.mapbox.Center(lat=capital['latitude'].values[0], lon=capital['longitude'].values[0])\n",
    "# Аналог с помощью словаря\n",
    "#map_center =                   dict(lat=capital['geo_lat'].values[0], lon=capital['geo_lon'].values[0])\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\", mapbox=dict(center=map_center, zoom=3))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    data_frame= df_estate.groupby(['state'], as_index=False)['target_clean'].median().sort_values(by=['target_clean'], ascending=False),\n",
    "    x='state',\n",
    "    y='target_clean',\n",
    "    color='state',\n",
    "    text='target_clean', \n",
    "    orientation='v',\n",
    "    title='Median value of the price from the state',\n",
    "    height=500, \n",
    "    width=1000   \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treemap_data = df_estate.groupby(\n",
    "    by='city',\n",
    "    as_index=False\n",
    ")[['target_clean']].median().sort_values(by=['target_clean'], ascending=False)[:101]\n",
    "\n",
    "#строим график\n",
    "fig = px.treemap(\n",
    "    data_frame=treemap_data, #DataFrame\n",
    "    path=['city'], #категориальный признак, для которого строится график\n",
    "    values='target_clean', #параметр, который сравнивается\n",
    "    height=500, #высота\n",
    "    width=1500, #ширина\n",
    "    title='Median price by city-100' #заголовок    \n",
    ")\n",
    "\n",
    "#отображаем график\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объявления представлены из 39 разных штатов(из 50), болше всего предложиний из штата Флорида(FL) - более 103 тыс. и штата Техас(TX) - более 81 тыс.\n",
    "\n",
    "Что касаемое медианной цены недвижимости, то в топ-лидерах штаты:\n",
    "\n",
    "- Нью-Йорк(NY) - $726850\n",
    "- Массачусетс(MA) - $674700\n",
    "- Калифорния(CA) - $659000\n",
    "- Округ Колумбия(DC) - $629000\n",
    "\n",
    "Наименьшая стоимость:\n",
    "\n",
    "- OT - $50000\n",
    "- Алабама(AL) - $72000\n",
    "- Монтана(MT) - $82900\n",
    "- Оклахома(OK) - $114750\n",
    "\n",
    "Предполагаем о влиянии данного признака на цену недвижимости. \n",
    "\n",
    "Также стоит отметить топ-город по цене недвижимости:\n",
    "\n",
    "- Saratoga(штат Калифорния) - $2998000\n",
    "- Los Altos Hills(штат Калифорния) - $2980000\n",
    "- New Richmond(штат Огайо) - $2980000\n",
    "- Highway 30(штат Оклахома) - $2900000\n",
    "- Bow Mar(штат Колорадо) - $2750000\n",
    "- Rancho Park(штат Калифорния) - $2750000\n",
    "- Santa Monica(штат Калифорния) - $2695000\n",
    "- Sunset Beach(штат Серерная Каролина) - $2595000\n",
    "\n",
    "Наименьшая стоимость:\n",
    "\n",
    "- Ford Hancock(штат Техас) - $35000\n",
    "- Wayland(штат Миссури) - $35200\n",
    "- Mem () - $35900\n",
    "- Minnie(штат Флорида) - $37000\n",
    "- Shillington(штат Пенсильвания) - $38566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **beds - target_clean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=df_estate,\n",
    "    x='beds',    \n",
    "    title='Distribution of the beds',\n",
    "    text_auto=True,    \n",
    "    height=500,    \n",
    "    width=1000      \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    data_frame=df_estate.groupby(['beds'], as_index=False)['target_clean'].median(),\n",
    "    x='beds',   \n",
    "    y='target_clean',\n",
    "    color='beds',\n",
    "    title='Scatterplot between beds and target_clean(median)',       \n",
    "    height=500,    \n",
    "    width=1000,\n",
    "    size='target_clean'       \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим зависимость цены недвижимости от числа комнат - при чём до трёх включительно разница небольшая, после трёх видно ускорение в увеличении стоимости: особенно скачок после 4 комант, после 10 комнат. Опять же с 7 до 10 комнат разница несущественна - возможна связано, с небольшой разницей в общей площади дома и более меньшими площадями самих комнат. Топовое количество комнат - 3(более 198 тыс. объектов). Интересны объекты с 1/9/13 комнатами, где стоимость ниже аналогичных объектов с меньшим количеством комнат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **baths - target_clean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=df_estate,\n",
    "    x='baths',    \n",
    "    title='Distribution of the baths',\n",
    "    text_auto=True,    \n",
    "    height=500,    \n",
    "    width=1000,\n",
    "    nbins=20     \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    data_frame=df_estate.groupby(['baths'], as_index=False)['target_clean'].median(),\n",
    "    x='baths',   \n",
    "    y='target_clean',\n",
    "    color='baths',\n",
    "    title='Scatterplot between baths and target_clean(median)',       \n",
    "    height=500,    \n",
    "    width=1000,\n",
    "    size='target_clean'       \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Картинка неоднозначная - да, видим, что при увеличении количества ванных комнат растёт и медианная стоимость недвижимости, но интересны некоторые значения, которые выпадают из общего тренда:\n",
    "\n",
    "- участок с 10 по 12 включительно цена наоборот снижается - в том числе за счёт того, что снижается средняя площадь недвижимости как увидим позже\n",
    "- значения baths с 17/19/20 - через чур низкие - объясняется тем, что данные объекты расположены в штатах с более низкой медианной стоимостью недвижимости(Техас(TX), Флорида(FL)). Представлено немного объектов\n",
    "- значения 5.2, 4.75, 3.2 - через чур высокие - недвижимость из априори дорогих штатов. Представлено немного объектов\n",
    "- значения 6.5/8.5 - ниже - из-за того, что основная масса предложений в данной категории из штата Флорида(FL), а это недвижимости типа single family. Если посмотреть на медианную площадь данной недвижимости в штате Флорида и в других штатах, то мы увидим, что во Флориде она существенно меньше(3544 против 6590 sqft). Представлено немного объектов\n",
    "\n",
    "Основная доля объектов приходится с количеством ванных комнат от 2 до 4(медианное значение 2.25). Зависимость прослеживается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Медианная площадь дома 6.5/8.5 в других штатах - {df_estate[(df_estate[\"state\"]!=\"FL\") & ((df_estate[\"propertyType\"]==\"single family\")) & ((df_estate[\"baths\"]==8.50) | (df_estate[\"baths\"]==6.50))][\"sqft\"].median()}')\n",
    "\n",
    "print(f'Медианная площадь дома 6.5/8.5 в штатае FL - {df_estate[(df_estate[\"state\"] == \"FL\") & ((df_estate[\"propertyType\"]==\"single family\")) & ((df_estate[\"baths\"]==8.50)|(df_estate[\"baths\"]==6.50))][\"sqft\"].median()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **sqft - target_clean**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признак sqft имеет большой разброс(квадратичное отклонение 772338) - от 0 до 456602500 квадратных фунтов, при этом медианное значение равняется 1800. Пока никаких манипуляций с ним производить не будем, но учтём этот момент на будущее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['sqft'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    data_frame=df_estate.groupby(['age'], as_index=False)['sqft'].median(),\n",
    "    y='sqft',   \n",
    "    x='age',\n",
    "    color='sqft',\n",
    "    title='Scatterplot between age and sqft(median)',       \n",
    "    height=500,    \n",
    "    width=1000,\n",
    "    #size='sqft'       \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интересную информацию даёт график зависимости медианной площади недвижимости в зависимости от возраста:\n",
    "\n",
    "- старые дома(более 100-120 лет - старше 1900 г) имеют более большой диапазон в площадях домов, они разнятся от года к году, много домов большой площади, далее они приходят к более единой медианной площади разнявшихся несильно. Отметим, что если необходим дом с большой площадью, то стоит его подыскивать именно в этой категории.\n",
    "- период 90-100 лет(1922-1932 гг) - замечено увеличение площадей дома(в среднем с 1560 до 1800) - как раз в Америке совпал данный период с бурным экономическим ростом, закончившись великой депрессией 1929г.\n",
    "- период 80-90 лет(1932-1942 гг) - снижение медианных площадей(до 1344) опять же вызванных экономическим кризисом\n",
    "- период 58-80 (1942-1964 гг) - бурный рост площадей до 1700 совпавшим с экономическим ростом\n",
    "- периоды 49-58(1964-1980 гг) - снижение до 1426(1973 г. - нефтяной кризис), 1975-1981(снижение до 1477 с ростом в 1976)\n",
    "- период 20-49(1981-2000гг) - бурный рост до 2130\n",
    "- период 0-20 (2000-2022гг) - наблюдалось снижение(до 1700) на фоне кризиса в Америке в нулевых, далее взлёт до 2506 и коррекция к 2018 с тенденцией на уменьшение площади\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    data_frame=df_estate.groupby(['baths'], as_index=False)['sqft'].median(),\n",
    "    y='sqft',   \n",
    "    x='baths',\n",
    "    color='baths',\n",
    "    title='Scatterplot between baths and sqft(median)',       \n",
    "    height=500,    \n",
    "    width=1000,\n",
    "    size='sqft'       \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее мы рассматривали зависимость целевой переменной от количества ванных комнат и выявили объекты, у которых стоимость недвижимости снижалось после достижения ванных комнат 9 и  до 15, и мы выдвигали гипотезу, что с ростом ванных комнат растёт и цена недвижимости, но при достижении числа 9(и вплоть до 15)начинается снижение медианной площади недвижимости - график подтвердил наши догадки.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    data_frame= df_estate.groupby(['propertyType'], as_index=False)['sqft'].median().sort_values(by=['sqft'], ascending=False),\n",
    "    x='propertyType',\n",
    "    y='sqft',\n",
    "    color='propertyType',\n",
    "    text='sqft', \n",
    "    orientation='v',\n",
    "    title='Median value of the sqft from the propertyType',\n",
    "    height=500, \n",
    "    width=1000   \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ожидаемые ТОП типов недвижимости по площадям:\n",
    "\n",
    "- multi family - 2200\n",
    "- traditional - 2144\n",
    "- single detached - 2054\n",
    "- single family - 1918\n",
    "\n",
    "Наимение это condo(1197), coop(1200), high rise(1260)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **stories -target_clean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=df_estate,\n",
    "    x='stories',    \n",
    "    title='Distribution of the stories',\n",
    "    text_auto=True,    \n",
    "    height=500,    \n",
    "    width=1000,\n",
    "    nbins=40     \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    data_frame=df_estate.groupby(['stories'], as_index=False)['target_clean'].median(),\n",
    "    x='stories',   \n",
    "    y='target_clean',\n",
    "    color='stories',\n",
    "    title='Scatterplot between stories and target_clean(median)',       \n",
    "    height=500,    \n",
    "    width=1000,\n",
    "    size='target_clean'       \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная часть недвижимости расположена на 2-3 этажах, медианное значение - 1 этаж, что вполне объяснимо так как это частные дома. Имеется плавная прямая зависимость стоимости от этажности строения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **heating-target_clean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=df_estate,\n",
    "    x='heating',    \n",
    "    title='Distribution of the heating',\n",
    "    text_auto=True,    \n",
    "    height=500,    \n",
    "    width=1000        \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    data_frame= df_estate.groupby(['heating'], as_index=False)['target_clean'].median().sort_values(by=['target_clean'], ascending=False),\n",
    "    x='heating',\n",
    "    y='target_clean',\n",
    "    color='heating',\n",
    "    text='target_clean', \n",
    "    orientation='v',\n",
    "    title='Median value of the target_clean from the heating',\n",
    "    height=500, \n",
    "    width=1000   \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобладающая часть недвижимость с отоплением типа forced air - медианная стоимость такой недвижимости - $337188, самая дорогая недвижимость с central air/central - $392450/$359000, а с электрическим отоплением(electric) наименьшая - $270000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **cooling - target_clean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=df_estate,\n",
    "    x='cooling',    \n",
    "    title='Distribution of the cooling',\n",
    "    text_auto=True,    \n",
    "    height=500,    \n",
    "    width=1000        \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    data_frame= df_estate.groupby(['cooling'], as_index=False)['target_clean'].median().sort_values(by=['target_clean'], ascending=False),\n",
    "    x='cooling',\n",
    "    y='target_clean',\n",
    "    color='cooling',\n",
    "    text='target_clean', \n",
    "    orientation='v',\n",
    "    title='Median value of the target_clean from the cooling',\n",
    "    height=500, \n",
    "    width=1000   \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **parking - target_clean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=df_estate,\n",
    "    x='parking',    \n",
    "    title='Distribution of the parking',\n",
    "    text_auto=True,    \n",
    "    height=500,    \n",
    "    width=1000        \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    data_frame= df_estate.groupby(['parking'], as_index=False)['target_clean'].median().sort_values(by=['target_clean'], ascending=False),\n",
    "    x='parking',\n",
    "    y='target_clean',\n",
    "    color='parking',\n",
    "    text='target_clean', \n",
    "    orientation='v',\n",
    "    title='Median value of the target_clean from the parking',\n",
    "    height=500, \n",
    "    width=1000   \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более 2/3 недвижимости в признаке parking представлены attached(пристроенный гараж) с медианной стоимостью $32500. Интересно, что имеется недвижимость без парковочного места(0) и её стоимость $1100000, что странно, но это объясняется тем, что данная недвижимость представлена из престижного штата Калифорния(CA). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **rating_mean/schools_count - target_clean** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=df_estate,\n",
    "    x='rating_mean',    \n",
    "    title='Distribution of the rating_mean',\n",
    "    text_auto=True,    \n",
    "    height=500,    \n",
    "    width=1000,\n",
    "    nbins=40      \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=df_estate,\n",
    "    x='schools_count',    \n",
    "    title='Distribution of the schools_count',\n",
    "    text_auto=True,    \n",
    "    height=500,    \n",
    "    width=1000,\n",
    "    nbins=40        \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    data_frame=df_estate.groupby(['rating_mean'], as_index=False)['target_clean'].median().sort_values(by=['target_clean'], ascending=False),\n",
    "    x='rating_mean',   \n",
    "    y='target_clean',\n",
    "    color='rating_mean',\n",
    "    title='Scatterplot between rating_mean and target_clean(median)',       \n",
    "    height=500,    \n",
    "    width=1000,\n",
    "    size='target_clean'       \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    data_frame= df_estate.groupby(['state'], as_index=False)['rating_mean'].median().sort_values(by=['rating_mean'], ascending=False),\n",
    "    x='state',\n",
    "    y='rating_mean',\n",
    "    color='state',\n",
    "    text='rating_mean', \n",
    "    orientation='v',\n",
    "    title='Median value of the rating_mean from the state',\n",
    "    height=500, \n",
    "    width=1000   \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    data_frame=df_estate.groupby(['schools_count'], as_index=False)['target_clean'].median().sort_values(by=['target_clean'], ascending=False),\n",
    "    x='schools_count',   \n",
    "    y='target_clean',\n",
    "    color='schools_count',\n",
    "    title='Scatterplot between schools_count and target_clean(median)',       \n",
    "    height=500,    \n",
    "    width=1000,\n",
    "    size='target_clean'       \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['rating_mean'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate['distance_mean'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Средний рейтинг школ - 5.18, медианное количество школ около объекта недвижимости - 5.18, расстояние до которых 1.75 мили. От рейтинга школы зависимость целевой переменной прослеживается, но слабая, а вот количество школ в общем не влияет на цену недвижимости, но имеются отдельные выбросы не входящий в общий тренд."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **age - target_clean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data_frame=df_estate,\n",
    "    x='age',    \n",
    "    title='Distribution of the age',\n",
    "    text_auto=True,    \n",
    "    height=500,    \n",
    "    width=1000,\n",
    "    nbins=50,\n",
    "    marginal='box'     \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    data_frame=df_estate.groupby(['age'], as_index=False)['target_clean'].median(),\n",
    "    x='age',   \n",
    "    y='target_clean',\n",
    "    color='age',\n",
    "    title='Scatterplot between age and target_clean(median)',       \n",
    "    height=500,    \n",
    "    width=1000,\n",
    "    #size='target_clean'       \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По возрасту недвижимости преобладают объекты 30-39 лет(медианный возраст - 38 лет). В среднем с увеличением возраста недвижимости прослеживается тенденция к уменьшению её стоимости, но довоенная недвижимость(80-87 лет) также держится в цене. Вообщем, что касаемо объектов старше 100 лет, то их можно разделить на два класса - те которые имеют цену ниже рыночной - возможно это объекты требующие вложений/реставрации/расположенных в непристижных районах и второй класс - те, которые стоят существенно дороже - какие-то экслюзивные варианты, также имеющие большую площадь/либо большой земельный участок/лучшую локацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишем типичный объект из нашего датасета: этот объект является одноэтажным(stories) домом для одной семьи(single family) площадью 1800 квадратных фунтов(sqft) и площадью участка(lotsize) 10890 квадратных фунтов, имеется пристроенный гараж(attached), принудительное воздушное отопление(forced air), 3 комнаты(beds) и 2.5 ванны комнаты(baths), возраст дома 38 лет, рядом(около 1.75 миль) находятся 3 школы(schools_count) со средним рейтингом (rating_mean) равным 5.17. Объект в статусе для продажи(for sale) и оценивается около $327018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Статистический анализ данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим гипотезу: \"Влияет ли количество школ (>3) на цену?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu, normaltest, pearsonr, spearmanr\n",
    "# гипотеза: \"Влияет ли количество школ (>3) на цену?\"\n",
    "group_many_schools = df_estate[df_estate['schools_count'] > 3]['target_clean']\n",
    "group_few_schools = df_estate[df_estate['schools_count'] <= 3]['target_clean']\n",
    "\n",
    "stat, p_value = mannwhitneyu(group_many_schools, group_few_schools, alternative='greater')\n",
    "\n",
    "print(f\"1. Тест Манна-Уитни (Влияние школ):\")\n",
    "print(f\"   Средняя цена (>3 школ): ${group_many_schools.mean():,.0f}\")\n",
    "print(f\"   Средняя цена (<=3 школ): ${group_few_schools.mean():,.0f}\")\n",
    "print(f\"   P-value: {p_value:.4e}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Вывод: Количество школ статистически значимо влияет на цену (отвергаем H0).\")\n",
    "else:\n",
    "    print(\"Вывод: Значимой разницы не найдено.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним корреляцию Пирсона (линейная) и Спирмена (ранговая/нелинейная) для ключевого признака sqft. Если Спирмен > Пирсона, значит связь нелинейная."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'sqft'\n",
    "target = 'target_clean'\n",
    "\n",
    "# Удаляем выбросы для честности теста\n",
    "clean_data = df_estate[(df_estate[feature] < 20000) & (df_estate[target] < 5000000)]\n",
    "\n",
    "corr_pearson, p_pearson = pearsonr(clean_data[feature], clean_data[target])\n",
    "corr_spearman, p_spearman = spearmanr(clean_data[feature], clean_data[target])\n",
    "\n",
    "print(f\"Анализ природы зависимости (sqft vs price):\")\n",
    "print(f\"   Корреляция Пирсона (Линейная): {corr_pearson:.3f}\")\n",
    "print(f\"   Корреляция Спирмена (Нелинейная): {corr_spearman:.3f}\")\n",
    "\n",
    "diff = corr_spearman - corr_pearson\n",
    "print(f\"   Разница: {diff:.3f}\")\n",
    "\n",
    "if diff > 0.05:\n",
    "    print(\"   Ранговая корреляция заметно выше линейной.\")\n",
    "    print(\"   Это статистически доказывает, что зависимость цены от площади не является строго линейной\")\n",
    "else:\n",
    "    print(\"   Зависимость близка к линейной.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Проверим статистически Влияние рейтинга школ на цену недвижимости\n",
    "\n",
    "Ранее мы выяснили, что количество школ влияет на цену. \n",
    "Теперь проверим, влияет ли средний рейтинг школ (rating_mean).\n",
    "Разделим дома на две группы: \"Хорошие школы\" (rating > 7) и \"Обычные\" (rating <= 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим дома на две группы: \"Хорошие школы\" (rating > 7) и \"Обычные\" (rating <= 7).\n",
    "\n",
    "high_rating = df_estate[df_estate['rating_mean'] >= 7]['target_clean']\n",
    "low_rating = df_estate[df_estate['rating_mean'] < 7]['target_clean']\n",
    "\n",
    "# Тест Манна-Уитни\n",
    "stat_school, p_school = mannwhitneyu(high_rating, low_rating, alternative='greater')\n",
    "\n",
    "print(f\"Тест влияния рейтинга школ (High > 7 vs Low < 7):\")\n",
    "print(f\"   Медианная цена (Top Schools): ${high_rating.median():,.0f}\")\n",
    "print(f\"   Медианная цена (Avg Schools): ${low_rating.median():,.0f}\")\n",
    "print(f\"   P-value: {p_school:.4e}\")\n",
    "\n",
    "if p_school < 0.05:\n",
    "    print(\"   Вывод: Дома рядом с высокорейтинговыми школами стоят статистически значимо дороже.\")\n",
    "    print(\"   Это подтверждает важность признака `rating_mean` для модели.\")\n",
    "else:\n",
    "    print(\"   Вывод: Значимой разницы не найдено.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Проверим гипотезу: \"Старый дом с ремонтом стоит дороже старого дома без ремонта\".\n",
    "\n",
    "Возьмем дома старше 30 лет.\n",
    "Группа 1: Был ремонт за последние 15 лет (age_remodeled < 15).\n",
    "Группа 2: Ремонта не было давно (age_remodeled > 30)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_houses = df_estate[df_estate['age'] > 30]\n",
    "\n",
    "renovated = old_houses[old_houses['age_remodeled'] < 15]['target_clean']\n",
    "original = old_houses[old_houses['age_remodeled'] > 30]['target_clean']\n",
    "\n",
    "if len(renovated) > 0 and len(original) > 0:\n",
    "    stat_renov, p_renov = mannwhitneyu(renovated, original, alternative='greater')\n",
    "\n",
    "    print(f\"5. Тест эффективности реновации (для домов старше 30 лет):\")\n",
    "    print(f\"   Медиана (Недавний ремонт): ${renovated.median():,.0f}\")\n",
    "    print(f\"   Медиана (Без ремонта):     ${original.median():,.0f}\")\n",
    "    \n",
    "    # Считаем \"Премию за ремонт\" в процентах\n",
    "    premium = (renovated.median() - original.median()) / original.median() * 100\n",
    "    \n",
    "    print(f\"   P-value: {p_renov:.4e}\")\n",
    "    \n",
    "    if p_renov < 0.05:\n",
    "        print(f\"   Вывод: Реновация повышает стоимость старого жилья. 'Премия за ремонт' составляет ~{premium:.1f}%.\")\n",
    "    else:\n",
    "        print(\"   Вывод: Реновация не дает статистически значимого прироста цены.\")\n",
    "else:\n",
    "    print(\"   Недостаточно данных для проверки гипотезы о реновации.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Формируем автоматическую визуализацию датасета с помощью D-Tale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Интерактивный аудит данных\n",
    "\n",
    "if GENERATE_DTALE:\n",
    "    print(\"Очистка старых процессов D-Tale...\")\n",
    "    global_state.cleanup()\n",
    "    dtale_app.USE_COLAB = True \n",
    "    d = dtale.show(df_estate)\n",
    "    print(f\"Интерактивный отчет доступен по ссылке: {d._main_url}\")\n",
    "    logging.info(f\"Интерактивный отчет D-Tale доступен по ссылке: {d._main_url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Отбор признаков**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашем датасете сформировано 24 признака, 9 из которых имеют тип object, закодируем их, далее полученные признаки будем приводить к одному виду - нормировать/стандартизировать т.к. числовые величины, которыми описываются признаки имеют разный порядок и более большие по модулю сбивают модель в свою сторону. Для начала построим матрицу корреляции для обнаружения линейной связи между целевым признаком и предикторами, а также признаки, которые сильно закоррелированные между собой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estate.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# строим матрицу корреляции\n",
    "fig, axes = plt.subplots(figsize=(24, 10))\n",
    "sns.heatmap(round(df_estate.corr(method='spearman', numeric_only=True), 2), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим корреляцию целевого признака с:\n",
    "- price_sqft - 0.66\n",
    "- sqft - 0.48\n",
    "- baths - 0.39\n",
    "- rating_mean - 0.33\n",
    "- beds - 0.24\n",
    "\n",
    "заметны сильноскореллированные признаки, такие как, age/age_remodeled(0.83)  - часть из них придётся убрать. Остаётся вопрос о спорном признаке price_sqft - через который как бы происходит утечка информации о целевом признаке, поэтому удалим его. \n",
    "\n",
    "В наших данных имеются несколько признаков ниже, которые сообщают нам одну и ту же информацию, а именно о локации объекта недвижимости - данный фактор существенен для определения стоимости недвижимости:\n",
    "\n",
    "- city\n",
    "- state\n",
    "- zipcode\n",
    "- county\n",
    "- latitude\n",
    "- longitude\n",
    "- street\n",
    "\n",
    "Все признаки типа object, zipcode, latitude, longitude - оставим эти три признака опеределяющих местоположение, остальные удалим. Также стоит заметить, что признак street использовать не получиться из-за большого количества уникальных значений и при кодировании получим очень много признаков.\n",
    "\n",
    "Подготовим данные: удалим часть предикторов, необходимые закодируем, далее выборку разделим на тренировочную и тестовую в соотношении 70/30, далее нормализуем данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_estate.drop(['city', 'street', 'state', 'county', 'age_remodeled', 'price_sqft', 'target_clean', 'target_clean_log'], axis=1)\n",
    "y = df_estate['target_clean']\n",
    "# Преобразуем zipcode в целочисленный тип\n",
    "X['zipcode'] = pd.to_numeric(X['zipcode'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "object_columns = [s for s in X.columns if X[s].dtypes == 'object']\n",
    "X = pd.get_dummies(X, columns=object_columns, drop_first=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.30, \n",
    "    random_state=RANDOM_SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# инициализируем нормализатор RobustScaler\n",
    "r_scaler = preprocessing.RobustScaler()\n",
    "\n",
    "# кодируем исходный датасет\n",
    "X_train_scal = r_scaler.fit_transform(X_train)\n",
    "\n",
    "# Преобразуем промежуточный датасет в полноценный датафрейм для визуализации\n",
    "X_train_scal = pd.DataFrame(X_train_scal, columns=X_train.columns)\n",
    "X_test_scal = pd.DataFrame(r_scaler.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Предварительный анализ моделей и Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Линейная регрессия**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала сформируем базовую модель, которую будем улучшать базовая модель будет строиться на всех признаках (59), далее производим предсказание - на тренировочных и на тестовых данных и проверяем метрики. \n",
    "\n",
    "В качестве метрик используем три метрики - MAE(используем больше для себя - для понимания в абсолютных величинах на сколько в среднем ошибается модель, данную величину легко сравнить с медианной стоимостью недвижимости), MSE и R2 для сравнения моделей между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_func(y_train, y_train_pred, y_test, y_test_predict, model_name=\"Unnamed Model\", exec_time=None):\n",
    "    \"\"\"\n",
    "    Вычисляет метрики, пишет в лог и сохраняет в таблицу экспериментов.\n",
    "    Добавлен учет времени выполнения.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Вычисляем метрики\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_predict)\n",
    "    \n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    mse_test = mean_squared_error(y_test, y_test_predict)\n",
    "    \n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test = r2_score(y_test, y_test_predict)\n",
    "    \n",
    "    # Форматирование времени (сек -> мин:сек)\n",
    "    if exec_time is not None:\n",
    "        if exec_time < 60:\n",
    "            time_str = f\"{exec_time:.2f} sec\"\n",
    "        else:\n",
    "            mins, secs = divmod(exec_time, 60)\n",
    "            time_str = f\"{int(mins)} min {int(secs)} sec\"\n",
    "    else:\n",
    "        time_str = \"N/A\"\n",
    "    \n",
    "    # 2. Формируем красивое сообщение для лога\n",
    "    log_msg = (\n",
    "        f\"\\n{'='*65}\\n\"\n",
    "        f\"MODEL: {model_name}\\n\"\n",
    "        f\"{'-'*65}\\n\"\n",
    "        f\"{'Metric':<10} | {'Train':>20} | {'Test':>20}\\n\"\n",
    "        f\"{'-'*65}\\n\"\n",
    "        f\"{'MAE':<10} | {mae_train:>20,.2f} | {mae_test:>20,.2f}\\n\"\n",
    "        f\"{'MSE':<10} | {mse_train:>20,.0f} | {mse_test:>20,.0f}\\n\"\n",
    "        f\"{'R2':<10} | {r2_train:>20.4f} | {r2_test:>20.4f}\\n\"\n",
    "        f\"{'-'*65}\\n\"\n",
    "        f\"{'Time':<10} | {time_str:>43}\\n\" # Выводим время внизу\n",
    "        f\"{'='*65}\\n\"\n",
    "    )\n",
    "    \n",
    "    # Вывод\n",
    "    print(log_msg)\n",
    "    logging.info(log_msg)\n",
    "    \n",
    "    # 3. Сохраняем в глобальный список\n",
    "    EXPERIMENTS_DATA.append({\n",
    "        'Model': model_name,\n",
    "        'R2 Score (Train)': round(r2_train, 3),\n",
    "        'R2 Score (Test)': round(r2_test, 3),\n",
    "        'MAE (Train)': round(mae_train, 0),\n",
    "        'MAE (Test)': round(mae_test, 0),\n",
    "        'MSE (Train)': round(mse_train, 0),\n",
    "        'MSE (Test)': round(mse_test, 0),\n",
    "        'Execution Time': time_str  \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  # <--- ЗАПУСК ТАЙМЕРА\n",
    "model_base = LinearRegression()\n",
    "model_base.fit(X_train_scal, y_train)\n",
    "y_train_predict = model_base.predict(X_train_scal)\n",
    "y_test_predict = model_base.predict(X_test_scal)\n",
    "\n",
    "end_time = time.time()    # <--- ОСТАНОВКА ТАЙМЕРА\n",
    "elapsed_time = end_time - start_time\n",
    "metrics_func(y_train, y_train_predict, y_test, y_test_predict, model_name=\"LinearRegression (baseline)\", exec_time=elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовая модель показала не лучшие метрики, т.к. абсолютная ошибка в размере более $258703 является очень большой. \n",
    "\n",
    "Коэффицент детерминации не сильно различается для тренировочных (0.302) и тестовых данных (0.306), что говорит о том, что модель смогла уловить в должной мере зависимости, но скорее всего зависимость сложнее. \n",
    "\n",
    "Поробуем улучшить метрики, а именно обучить модель на отобранных лучших признаках, для этого используем RFECV (Рекурсивное исключение признаков с кросс-валидацией) — Алгоритм берет все признаки, обучает модель, выкидывает самый слабый признак, снова обучает, проверяет точность на кросс-валидации. И так, пока не найдет пик точности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Важно: используем subset данных (10%) для того чтобы модель не работала слишком долго\n",
    "RFECV_train_size=0.1\n",
    "# меньше 5 признаков не оставлять\n",
    "RFECV_min_features=5\n",
    "\n",
    "print(\"Запуск автоматического отбора признаков (RFECV)...\")\n",
    "\n",
    "# 1. Используем быстрый эстиматор (RandomForest с ограничениями)\n",
    "X_sample, _, y_sample, _ = train_test_split(X_train_scal, y_train, train_size=RFECV_train_size, random_state=RANDOM_SEED)\n",
    "\n",
    "estimator = RandomForestRegressor(n_estimators=50, max_depth=5, n_jobs=SAFE_N_JOBS, random_state=RANDOM_SEED)\n",
    "\n",
    "# 2. Настраиваем RFECV\n",
    "# step=1: удаляем по 1 признаку за шаг\n",
    "selector = RFECV(estimator, step=1, cv=KFold(3), scoring='r2', min_features_to_select=RFECV_min_features, n_jobs=SAFE_N_JOBS)\n",
    "selector.fit(X_sample, y_sample)\n",
    "\n",
    "print(f\"Оптимальное количество признаков: {selector.n_features_}\")\n",
    "logging.info(f\"Оптимальное количество признаков по RFECV: {selector.n_features_}\")\n",
    "\n",
    "\n",
    "# 3. Визуализация процесса\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlabel(\"Количество признаков\")\n",
    "plt.ylabel(\"R2 Score (Cross-Validation)\")\n",
    "plt.plot(range(5, len(selector.cv_results_['mean_test_score']) + 5), selector.cv_results_['mean_test_score'])\n",
    "plt.title(\"Зависимость точности от количества признаков\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 4. Сохраняем список лучших признаков\n",
    "best_features = X_train_scal.columns[selector.support_]\n",
    "print(f\"Отобранные признаки: {list(best_features)}\")\n",
    "logging.info(f\"Отобранные признаки: {list(best_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем количество признаков\n",
    "feature_num = selector.n_features_\n",
    "\n",
    "selector = feature_selection.SelectKBest(feature_selection.f_regression, k=feature_num)\n",
    "selector.fit(X_train_scal, y_train)\n",
    "best_features = selector.get_feature_names_out()\n",
    "best_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первой построим линейную регрессию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  # <--- ЗАПУСК ТАЙМЕРА\n",
    "model_1 = LinearRegression()\n",
    "model_1.fit(X_train_scal[best_features], y_train)\n",
    "y_train_predict = model_1.predict(X_train_scal[best_features])\n",
    "y_test_predict = model_1.predict(X_test_scal[best_features])\n",
    "end_time = time.time()    # <--- ОСТАНОВКА ТАЙМЕРА\n",
    "elapsed_time = end_time - start_time\n",
    "metrics_func(y_train, y_train_predict, y_test, y_test_predict, model_name=f\"LinearRegression (Top-{feature_num} features)\", exec_time=elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель, построенная на лучших признаках показала стабильно метрику на тесте $R^2=0.247$, что хуже, чем модель на всех признаках,  улавливаемая закономерность линейной модели стала еще ниже. \n",
    "\n",
    "\n",
    "Наблюдаемый эффект ($R^2Test > R^2 Train$) для линейных моделей свидетельствует об отсутствии переобучения, но подтверждает сильное недообучение (Underfitting). Линейная модель не обладает достаточной сложностью, чтобы охватить структуру данных, и результаты на тесте оказываются лучше лишь из-за статистических флуктуаций в распределении остатков.\n",
    "\n",
    "Дополнительно проверим гипотезу: Линейность vs Монотонность для обоснования выбора направления поиска лучшей модели\n",
    "Мы сравним корреляцию Пирсона (линейная) и Спирмена (ранговая/нелинейная) для ключевого признака sqft. Если Спирмен > Пирсона, значит связь нелинейная."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sqft = 'sqft'\n",
    "target_sqft = 'target_clean'\n",
    "\n",
    "# Удаляем выбросы\n",
    "clean_data = df_estate[(df_estate[feature_sqft] < 20000) & (df_estate[target_sqft] < 5000000)]\n",
    "\n",
    "corr_pearson, p_pearson = pearsonr(clean_data[feature_sqft], clean_data[target_sqft])\n",
    "corr_spearman, p_spearman = spearmanr(clean_data[feature_sqft], clean_data[target_sqft])\n",
    "\n",
    "print(f\"Анализ природы зависимости (sqft vs price):\")\n",
    "print(f\"   Корреляция Пирсона (Линейная): {corr_pearson:.3f}\")\n",
    "print(f\"   Корреляция Спирмена (Монотонная): {corr_spearman:.3f}\")\n",
    "\n",
    "diff = abs(corr_spearman - corr_pearson) # Берем модуль разницы\n",
    "\n",
    "# Интерпретация\n",
    "print(\"\\n ИНТЕРПРЕТАЦИЯ:\")\n",
    "if diff > 0.1: \n",
    "    print(\"   Разрыв между корреляциями велик. Присутствует сильная нелинейность.\")\n",
    "else:\n",
    "    print(\"   Разница между коэффициентами невелика, зависимость монотонная.\")\n",
    "\n",
    "print(f\"   Однако, значение корреляции ({corr_pearson:.2f}) является СРЕДНИМ.\")\n",
    "print(f\"   Это объясняет, почему простая Линейная Регрессия показывает низкий R2 (~{corr_pearson**2:.2f}).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Связь линейная (монотонная), но слабая (0.47) из-за фактора локации. В разных штатах наклон прямой (цена за метр) разный. \n",
    "Простая линейная регрессия усредняет это и ошибается. Градиентный бустинг же умеет находить локальные зависимости, поэтому его точность выше.\n",
    "\n",
    "Зависимость 'Площадь -> Цена' сильно варьируется в зависимости от локации, что линейная модель не может учесть без сложных взаимодействий признаков. Поэтому использование ансамблей (например, GradientBoosting) является оправданным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдём к модели деревьям решений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  # <--- ЗАПУСК ТАЙМЕРА\n",
    "tree_model = tree.DecisionTreeRegressor(random_state=RANDOM_SEED)\n",
    "tree_model.fit(X_train_scal[best_features], y_train)\n",
    "\n",
    "y_train_pred = tree_model.predict(X_train_scal[best_features])\n",
    "y_test_pred = tree_model.predict(X_test_scal[best_features])\n",
    "end_time = time.time()    # <--- ОСТАНОВКА ТАЙМЕРА\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "metrics_func(y_train, y_train_pred, y_test, y_test_pred, model_name=f\"DecisionTreeRegressor (Top-{feature_num} features)\", exec_time=elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данную модель построили на Top лучших признаках, очевидно, что есть явное переобучение модели, т.к. метрика на тесте $R^2=0.497$, а на трейне на тесте $R^2=0.865$. Поэтому соответственно подберём оптимальные параметры глубины дерева, используя метод локтя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Диапазон глубин для подбора оптимальной глубины\n",
    "max_depths = range(7, 20)\n",
    "\n",
    "def evaluate_depth(depth, X_train, X_test, y_train, y_test):\n",
    "    model = DecisionTreeRegressor(max_depth=depth, random_state=RANDOM_SEED)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    return round(r2_score(y_train, y_train_pred), 3), round(r2_score(y_test, y_test_pred), 3)\n",
    "\n",
    "def tree_depths_elbow(X_train, X_test, y_train, y_test):\n",
    "    print(f\"Запуск поиска оптимального глубины дерева (метод локтя)\")\n",
    "    \n",
    "    # 1. СЧИТАЕМ\n",
    "    results = joblib.Parallel(n_jobs=SAFE_N_JOBS)(\n",
    "        joblib.delayed(evaluate_depth)(d, X_train, X_test, y_train, y_test) for d in max_depths\n",
    "    )\n",
    "    \n",
    "    R_2_train, R_2_test = zip(*results)\n",
    "    \n",
    "    # 2. ЛОГИКА \"ПРАВИЛА ЛОКТЯ\" (ELBOW METHOD)\n",
    "    \n",
    "    # Находим абсолютный максимум на тесте\n",
    "    max_test_score = max(R_2_test)\n",
    "    \n",
    "    # Задаем допуск (Tolerance). \n",
    "    # Готовы пожертвовать 0.005 (0.5%) качества ради простоты модели.\n",
    "    # Если max = 0.636, то нас устроит любой результат >= 0.631\n",
    "    tolerance = 0.005 \n",
    "    threshold = max_test_score - tolerance\n",
    "    \n",
    "    optimal_depth = None\n",
    "    optimal_score = None\n",
    "    \n",
    "    # Ищем ПЕРВУЮ (минимальную) глубину, которая пробила порог\n",
    "    for i, score in enumerate(R_2_test):\n",
    "        if score >= threshold:\n",
    "            optimal_depth = list(max_depths)[i]\n",
    "            optimal_score = score\n",
    "            break # Останавливаемся, как только нашли \"достаточно хорошую\" глубину\n",
    "            \n",
    "    print(f\"Абсолютный максимум R2: {max_test_score}\")\n",
    "    print(f\"Выбрана оптимальная глубина (с учетом допуска): {optimal_depth} (R2: {optimal_score})\")\n",
    "    logging.info(f\"Выбрана оптимальная глубина дерева (с учетом допуска): {optimal_depth} (R2: {optimal_score})\")\n",
    "\n",
    "    # 3. РИСУЕМ\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=list(max_depths), y=R_2_train, name='Train', line=dict(width=3, color='#ff7f0e')))\n",
    "    fig.add_trace(go.Scatter(x=list(max_depths), y=R_2_test, name='Test', line=dict(width=3, color='#1f77b4')))\n",
    "    \n",
    "    # Зеленая линия на ОПТИМАЛЬНОЙ глубине\n",
    "    fig.add_vline(x=optimal_depth, line_width=2, line_dash=\"dash\", line_color=\"green\")\n",
    "    \n",
    "    # Аннотация\n",
    "    fig.add_annotation(\n",
    "        x=optimal_depth, y=optimal_score,\n",
    "        text=f\"Optimal Elbow: {optimal_depth}<br>R²: {optimal_score}\",\n",
    "        showarrow=True, arrowhead=1, ax=0, ay=-40,\n",
    "        bgcolor=\"white\", bordercolor=\"green\"\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(title='Max Depth')\n",
    "    fig.update_yaxes(title='R² Score')\n",
    "    fig.update_layout(\n",
    "        title=f'Tree Depth Optimization (Elbow Rule, Tolerance={tolerance})', \n",
    "        height=600, \n",
    "        width=800\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return optimal_depth\n",
    "\n",
    "# Запуск\n",
    "best_depth = tree_depths_elbow(X_train_scal, X_test_scal, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберем оптимальную глубину деревьев (13) и построим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  # <--- ЗАПУСК ТАЙМЕРА\n",
    "tree_model = tree.DecisionTreeRegressor(random_state=RANDOM_SEED, max_depth=best_depth)\n",
    "tree_model.fit(X_train_scal[best_features], y_train)\n",
    "\n",
    "y_train_pred = tree_model.predict(X_train_scal[best_features])\n",
    "y_test_pred = tree_model.predict(X_test_scal[best_features])\n",
    "end_time = time.time()    # <--- ОСТАНОВКА ТАЙМЕРА\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "metrics_func(y_train, y_train_pred, y_test, y_test_pred, model_name=f\"DecisionTreeRegressor (Elbow method depth = {best_depth})\", exec_time=elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрики вновь улучшились, $R^2 - 0.62/0.54$, но обратим внимание, что метрика на тренировочных данных лучше более, чем на 13%, что может говорить о переобучении модели и возможно необходимо снизить глубину."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ансамблевые методы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первой ансамблевой моделью будет бэггинг и его разновидность - модель случайного леса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# бэггинг случайный лес\n",
    "start_time = time.time()  # <--- ЗАПУСК ТАЙМЕРА\n",
    "random_forest = ensemble.RandomForestRegressor(n_estimators=200,\n",
    "                                               max_depth=16,                                               \n",
    "                                               criterion='squared_error',\n",
    "                                               random_state=RANDOM_SEED)\n",
    "\n",
    "random_forest.fit(X_train_scal[best_features], y_train)\n",
    "\n",
    "y_train_pred = random_forest.predict(X_train_scal[best_features])\n",
    "y_test_pred = random_forest.predict(X_test_scal[best_features])\n",
    "end_time = time.time()    # <--- ОСТАНОВКА ТАЙМЕРА\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "metrics_func(y_train, y_train_pred, y_test, y_test_pred, model_name=f\"RandomForestRegressor (Top-{feature_num} features)\", exec_time=elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрики вновь улучшились, $R^2 - 0.71/0.61$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующая модель градиентного бустинга - HistGradientBoostingRegressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем объект класса градиентный бустинг\n",
    "\n",
    "start_time = time.time()  # <--- ЗАПУСК ТАЙМЕРА\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(\n",
    "    max_iter=500,              # Вместо n_estimators\n",
    "    max_depth=9,               \n",
    "    learning_rate=0.01,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Обучаем модель\n",
    "hgb.fit(X_train_scal[best_features], y_train)\n",
    "\n",
    "# Формируем предсказание\n",
    "y_train_pred = hgb.predict(X_train_scal[best_features])\n",
    "y_test_pred = hgb.predict(X_test_scal[best_features])\n",
    "end_time = time.time()    # <--- ОСТАНОВКА ТАЙМЕРА\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "metrics_func(y_train, y_train_pred, y_test, y_test_pred, model_name=f\"HistGradientBoostingRegressor (Top-{feature_num} features)\", exec_time=elapsed_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрики вновь улучшились, $R^2 - 0.527/0.527$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё одна модель бустинга CatBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# модель бустинга CatBoost\n",
    "start_time = time.time()  # <--- ЗАПУСК ТАЙМЕРА\n",
    "catmodel = CatBoostRegressor(random_state=RANDOM_SEED, verbose=False)\n",
    "catmodel.fit(X_train_scal[best_features], y_train)\n",
    "\n",
    "y_train_pred  = catmodel.predict(X_train_scal[best_features])\n",
    "y_test_pred = catmodel.predict(X_test_scal[best_features])\n",
    "end_time = time.time()    # <--- ОСТАНОВКА ТАЙМЕРА\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "metrics_func(y_train, y_train_pred, y_test, y_test_pred, model_name=f\"CatBoostRegressor (Top-{feature_num} features)\", exec_time=elapsed_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого в фаворитах оказываются модели - случайный лес($R^2=0.71/0.61$), градиентный бустинг($R^2=0.53/0.53$) и CatBoost($R^2=0.62/0.61$). Но, вспомним о том, на каких признаках были построены данные модели - 16 лучших предикторов отобранных с помощью RFECV. Попытаемся отобрать важные признаки альтернативным способом.\n",
    "\n",
    " Воспользуемся CatBoost для построения модели на всех признаках, а далее отберём топ-предикторы согласно их весам на основе важности признаков CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  # <--- ЗАПУСК ТАЙМЕРА\n",
    "catmodel = CatBoostRegressor(random_state=RANDOM_SEED, verbose=False)\n",
    "catmodel.fit(X_train_scal, y_train)\n",
    "\n",
    "y_train_pred  = catmodel.predict(X_train_scal)\n",
    "y_test_pred = catmodel.predict(X_test_scal)\n",
    "end_time = time.time()    # <--- ОСТАНОВКА ТАЙМЕРА\n",
    "elapsed_time = end_time - start_time\n",
    "metrics_func(y_train, y_train_pred, y_test, y_test_pred, model_name=\"CatBoostRegressor (All features)\", exec_time=elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрики CatBoost для построения модели на всех признаках улучшились и составили $R^2 - 0.778/0.76$\n",
    "\n",
    "Попробуем повысить метрики за счет альтернативного определения количества признаков, и выберем лучшие признаки с помощью CatBoost, который использует более продвинутые методы оценки важности признаков (Feature Importance), чем стандартный RandomForest (часто переоценивает признаки с большим количеством уникальных значений)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Запуск отбора признаков на основе важности CatBoost...\")\n",
    "\n",
    "# 1. Обучаем CatBoost на всех данных (это быстро и точно)\n",
    "# Используем параметры для скорости (меньше итераций)\n",
    "selector_model = CatBoostRegressor(\n",
    "    iterations=500, \n",
    "    random_state=RANDOM_SEED, \n",
    "    verbose=0\n",
    ")\n",
    "selector_model.fit(X_train_scal, y_train)\n",
    "\n",
    "# 2. Используем SelectFromModel\n",
    "# threshold='median' выберет топ-50% признаков\n",
    "# threshold='1.25*mean' оставит только самые сильные\n",
    "# prefit=True говорит, что модель уже обучена\n",
    "selector = SelectFromModel(selector_model, threshold='median', prefit=True)\n",
    "\n",
    "# Получаем список лучших признаков\n",
    "selected_mask = selector.get_support()\n",
    "best_features_cat = X_train_scal.columns[selected_mask]\n",
    "\n",
    "print(f\"Отобрано признаков: {len(best_features_cat)}\")\n",
    "print(f\"Список признаков: {list(best_features_cat)}\")\n",
    "logging.info(f\"Отобрано признаков с помощью CatBoost: {len(best_features_cat)}\")\n",
    "logging.info(f\"Список признаков: {list(best_features_cat)}\")\n",
    "\n",
    "\n",
    "# 3. Визуализация (чтобы убедиться)\n",
    "feature_imp = pd.DataFrame({\n",
    "    'Feature': X_train_scal.columns,\n",
    "    'Importance': selector_model.get_feature_importance()\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Выводим топ-20\n",
    "import plotly.express as px\n",
    "fig = px.bar(feature_imp.head(20), x='Importance', y='Feature', orientation='h', \n",
    "             title=\"Feature Importance (CatBoost Full Data)\")\n",
    "fig.show()\n",
    "\n",
    "new_best = best_features_cat\n",
    "features_weight_num = len(best_features_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим по степени значимости на первом месте площадь недвижимости(sqft). Далее построим отмеченные ранее модели на новых топ признаках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  # <--- ЗАПУСК ТАЙМЕРА\n",
    "catmodel = CatBoostRegressor(random_state=RANDOM_SEED, verbose=False)\n",
    "catmodel.fit(X_train_scal[new_best], y_train)\n",
    "\n",
    "y_train_pred  = catmodel.predict(X_train_scal[new_best])\n",
    "y_test_pred = catmodel.predict(X_test_scal[new_best])\n",
    "end_time = time.time()    # <--- ОСТАНОВКА ТАЙМЕРА\n",
    "elapsed_time = end_time - start_time\n",
    "metrics_func(y_train, y_train_pred, y_test, y_test_pred, model_name=f\"CatBoostRegressor (Top-{features_weight_num} features)\", exec_time=elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрики CatBoost для построения модели на 30 лучших признаках немного снизились и составили $R^2 - 0.778/0.76$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  # <--- ЗАПУСК ТАЙМЕРА\n",
    "random_forest = ensemble.RandomForestRegressor(n_estimators=200,\n",
    "                                               max_depth=13,                                               \n",
    "                                               criterion='squared_error',\n",
    "                                               random_state=RANDOM_SEED)\n",
    "\n",
    "random_forest.fit(X_train_scal[new_best], y_train)\n",
    "\n",
    "y_train_pred = random_forest.predict(X_train_scal[new_best])\n",
    "y_test_pred = random_forest.predict(X_test_scal[new_best])\n",
    "end_time = time.time()    # <--- ОСТАНОВКА ТАЙМЕРА\n",
    "elapsed_time = end_time - start_time\n",
    "metrics_func(y_train, y_train_pred, y_test, y_test_pred, model_name=f\"RandomForestRegressor (Top-{features_weight_num} features)\", exec_time=elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрики RandomForest для построения модели на 30 лучших признаках улучшились и составили $R^2 - 0.76/0.7$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# визуализируем важные признаки в дереве решений\n",
    "oo = pd.DataFrame([random_forest.feature_importances_], columns=X_train_scal[new_best].columns)\n",
    "fig = px.bar( \n",
    "    x=list(oo.loc[0].sort_values(ascending=False).index),\n",
    "    y=round(oo.loc[0].sort_values(ascending=False), 2),\n",
    "    text_auto=True,\n",
    "    title='ТОП-11 features for RandomForest',\n",
    "    height=500, \n",
    "    width=1000,\n",
    "    labels={'x':'feature_importances', 'y':'weight'}\n",
    "       \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем объект класса HistGradientBoostingRegressor(современная реализация)\n",
    "start_time = time.time()  # <--- ЗАПУСК ТАЙМЕРА\n",
    "hgb = HistGradientBoostingRegressor(\n",
    "    max_iter=500,              # Вместо n_estimators\n",
    "    max_depth=9,               \n",
    "    learning_rate=0.01,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Обучаем модель\n",
    "hgb.fit(X_train_scal[new_best], y_train)\n",
    "\n",
    "# Формируем предсказание\n",
    "y_train_pred = hgb.predict(X_train_scal[new_best])\n",
    "y_test_pred = hgb.predict(X_test_scal[new_best])\n",
    "end_time = time.time()    # <--- ОСТАНОВКА ТАЙМЕРА\n",
    "elapsed_time = end_time - start_time\n",
    "metrics_func(y_train, y_train_pred, y_test, y_test_pred, model_name=f\"HistGradientBoostingRegressor (Top-{features_weight_num} features)\", exec_time=elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрики HistGradientBoosting для построения модели на 30 лучших признаках улучшились и составили $R^2 - 0.660/0.657$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# визуализируем важные признаки в градинетном бустинге\n",
    "\n",
    "# 1. Считаем Permutation Importance\n",
    "# n_repeats=5 достаточно для быстрой оценки\n",
    "print(\"Расчет важности признаков (Permutation Importance)...\")\n",
    "result = permutation_importance(\n",
    "    hgb, \n",
    "    X_train_scal[new_best], \n",
    "    y_train, \n",
    "    n_repeats=5, \n",
    "    random_state=RANDOM_SEED, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 2. Собираем DataFrame\n",
    "# Используем result.importances_mean (среднее падение метрики)\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X_train_scal[new_best].columns,\n",
    "    'Importance': result.importances_mean\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# 3. Рисуем (стиль Plotly)\n",
    "fig = px.bar( \n",
    "    importance_df,\n",
    "    x='Feature',\n",
    "    y='Importance',\n",
    "    text_auto='.4f',\n",
    "    title=f'Permutation Importance for HistGradientBoosting (Top-{features_weight_num})',\n",
    "    height=500, \n",
    "    width=1000,\n",
    "    labels={'Importance':'Mean Accuracy Decrease'}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание матриц наблюдений в формате DMatrix\n",
    "dtrain = xgb.DMatrix(X_train_scal[new_best], label=y_train, feature_names=new_best.tolist())\n",
    "dtest = xgb.DMatrix(X_test_scal[new_best], label=y_test, feature_names=new_best.tolist())\n",
    "\n",
    "# Гиперпараметры модели\n",
    "xgb_pars = {'min_child_weight': 20, 'eta': 0.1, 'colsample_bytree': 0.9, \n",
    "            'max_depth': 6, 'subsample': 0.9, 'lambda': 1, 'nthread': -1, \n",
    "            'booster' : 'gbtree', 'eval_metric': 'rmse', 'objective': 'reg:squarederror'\n",
    "           }\n",
    "# Тренировочная и валидационная выборка\n",
    "watchlist = [(dtrain, 'train'), (dtest, 'valid')]\n",
    "\n",
    "start_time = time.time()  # <--- ЗАПУСК ТАЙМЕРА\n",
    "# Обучаем модель XGBoost\n",
    "model_xgb = xgb.train(\n",
    "    params=xgb_pars, #гиперпараметры модели\n",
    "    dtrain=dtrain, #обучающая выборка\n",
    "    num_boost_round=300, #количество моделей в ансамбле\n",
    "    evals=watchlist, #выборки, на которых считается матрица\n",
    "    early_stopping_rounds=20, #раняя остановка\n",
    "    maximize=False, #смена поиска максимума на минимум\n",
    "    verbose_eval=10 #шаг, через который происходит отображение метрик\n",
    ")\n",
    "\n",
    "y_train_predict = model_xgb.predict(dtrain)\n",
    "y_test_predict = model_xgb.predict(dtest)\n",
    "end_time = time.time()    # <--- ОСТАНОВКА ТАЙМЕРА\n",
    "elapsed_time = end_time - start_time\n",
    "metrics_func(y_train, y_train_predict, y_test, y_test_predict, model_name=f\"XGBRegressor (Top-{features_weight_num} features)\", exec_time=elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (9,9))\n",
    "xgb.plot_importance(model_xgb, ax = ax, height=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Оценка построенных моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Промежуточный вывод:**\n",
    " 1.  Простые линейные модели показывают среднее качество ($R^2 \\approx 0.30$), что говорит о сложной нелинейной природе данных.\n",
    " 2.  Базовые ансамбли (RandomForest/CatBoost) \"из коробки\" дают неплохой результат ($R^2 \\approx 0.75-0.78$).\n",
    " 3.  **Проблема:** Текущий подход требует ручной обработки признаков перед каждой моделью, что неудобно и чревато ошибками.\n",
    " 4.  Отбор признаков на полном наборе данных с помощью градиентного бустинга (CatBoost) оказался более эффективным. Меньшее количество признаков (11 против 16) дало более высокое качество, что говорит об успешном отсеивании шума. Это подтверждает, что для данной задачи качество признаков важнее их количества.\n",
    "\n",
    "**Дальнейшие шаги:** Для улучшения результата и создания надежного решения мы перейдем к использованию **Sklearn Pipelines**, добавим продвинутую обработку категорий (`TargetEncoder`) и проведем оптимизацию гиперпараметров.\n",
    "\n",
    "\n",
    "**Анализ результатов базовых моделей:**\n",
    "Мы протестировали простые подходы на очищенных данных.\n",
    "1.  **Провал автоматического отбора признаков:** Модель `LinearRegression (Top-25)` показала худший результат ($R^2=0.25$) по сравнению с моделью на всех признаках ($R^2=0.30$).\n",
    "*   *Причина:* Стандартные методы отбора (по корреляции) исключили географические признаки (`latitude`, `longitude`), так как их связь с ценой нелинейна. Это доказывает, что для недвижимости нельзя удалять гео-данные.\n",
    "2.  **Ограниченность One-Hot Encoding:** Базовые ансамбли (RF, CatBoost) показали результат $R^2 \\approx 0.60-0.75$. Они не смогли полностью раскрыть потенциал данных из-за высокой кардинальности признака `zipcode` (4000+ уникальных значений), который сложно эффективно закодировать стандартными методами.\n",
    "3.  **Проблема:** Текущий подход требует ручной обработки признаков перед каждой моделью, что неудобно и чревато ошибками.\n",
    "\n",
    "**Решение:** Для улучшения метрик мы переходим к использованию **Pipeline** с применением **Target Encoding** для почтовых индексов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предварительно находим строки с аномально большими остатками (выбросы)\n",
    "\n",
    "На основе анализа стандартизированных остатков (Z-score) мы выявиляем объекты с экстремально большой ошибкой предсказания.\n",
    "\n",
    "Для этого обучим быструю линейную модель с использованием *Target Encoding* и рассчитаем Z-оценки ошибок. Далее найдем конкретные примеры, где модель ошибается сильнее всего (экстремальные выбросы).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Подготовка данных\n",
    "# Убираем явно лишние поля, но оставляем категориальные, которые хотим закодировать\n",
    "cols_to_drop = ['city', 'street', 'state', 'county', 'age_remodeled', 'price_sqft', 'target_clean']\n",
    "X = df_estate.drop(cols_to_drop, axis=1)\n",
    "y = df_estate['target_clean']\n",
    "\n",
    "# 2. Определяем категориальные признаки автоматически\n",
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Кодируем признаки с использованием Target Encoding: {cat_cols}\")\n",
    "\n",
    "# 3. Применяем Target Encoding\n",
    "encoder = TargetEncoder(cols=cat_cols)\n",
    "X_encoded = encoder.fit_transform(X, y)\n",
    "\n",
    "# 4. Обучение быстрой модели (LinearRegression из sklearn)\n",
    "model = LinearRegression(n_jobs=SAFE_N_JOBS)\n",
    "model.fit(X_encoded, y)\n",
    "\n",
    "# 5. Расчет остатков (Residuals)\n",
    "y_pred = model.predict(X_encoded)\n",
    "\n",
    "# Считаем \"чистые\" остатки\n",
    "residuals = y - y_pred\n",
    "\n",
    "# Стандартизируем остатки (Z-score): (остаток - среднее) / стандартное_отклонение\n",
    "# Это аналог стьюдентизированных остатков для поиска выбросов\n",
    "residuals_std = (residuals - residuals.mean()) / residuals.std()\n",
    "\n",
    "# 6. Поиск выбросов\n",
    "max_error_idx = residuals_std.idxmax()\n",
    "min_error_idx = residuals_std.idxmin()\n",
    "\n",
    "print(\"\\n=== РЕЗУЛЬТАТЫ ПОИСКА ВЫБРОСОВ ===\")\n",
    "print(f\"Максимальное отклонение (Z-score): {residuals_std[max_error_idx]:.2f}\")\n",
    "print(f\"Минимальное отклонение (Z-score):  {residuals_std[min_error_idx]:.2f}\")\n",
    "\n",
    "# Вывод строк из оригинального датафрейма\n",
    "outliers = df_estate.loc[[max_error_idx, min_error_idx]]\n",
    "\n",
    "print(\"\\nСтроки с самыми большими аномалиями:\")\n",
    "display(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаляем найденные строки по их индексам\n",
    "df_estate = df_estate.drop(outliers.index, errors='ignore')\n",
    "\n",
    "print(f\"Удалено строк: {len(outliers)}\")\n",
    "print(f\"Новый размер датасета: {df_estate.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По результатам анализа остатков (`residuals`) были определены индексы объектов с максимальным положительным и отрицательным отклонением. Данные точки классифицированы как выбросы, не соответствующие общему распределению целевой переменной.\n",
    "\n",
    "В данной ячейке производится удаление этих объектов из датасета `df_estate`.\n",
    "* Используется параметр `errors='ignore'`, чтобы избежать ошибок при повторном запуске ячейки.\n",
    "\n",
    "**Решение:**\n",
    "Выбираем **консервативный подход**: вместо автоматического удаления всех данных за пределами 3-х сигм, мы удаляем только самые явные, проверенные аномалии. Это позволяет избавиться от шума, сохранив при этом сложные, но валидные примеры для обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение с использованием Pipeline с применением Target Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ПОДГОТОВКА ДАННЫХ ДЛЯ ВСЕХ МОДЕЛЕЙ (SINGLE SOURCE OF TRUTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ПОДГОТОВКА ДАННЫХ ДЛЯ ВСЕХ МОДЕЛЕЙ (SINGLE SOURCE OF TRUTH)\n",
    "\n",
    "print(\"Начинаем подготовку данных для pipeline ML...\")\n",
    "\n",
    "# 1. Удаляем пропуски в цели\n",
    "df_model = df_estate.dropna(subset=['target_clean']).copy()\n",
    "\n",
    "# 2. Глобальная очистка выбросов (Одни правила для всех pipelines)\n",
    "df_model = df_model[\n",
    "    (df_model['sqft'] < 20000) & (df_model['sqft'] > 10) & \n",
    "    (df_model['lotsize'] < 500000) & \n",
    "    (df_model['distance_mean'] < 50) \n",
    "]\n",
    "\n",
    "# Сбрасываем индекс\n",
    "df_model = df_model.reset_index(drop=True)\n",
    "\n",
    "# 3. Формируем X и y\n",
    "cols_to_drop = ['target_clean', 'target_clean_log', 'price_sqft', 'street', 'city', 'county'] \n",
    "X = df_model.drop(cols_to_drop, axis=1, errors='ignore')\n",
    "y = df_model['target_clean']\n",
    "\n",
    "# 4. Split (Фиксируем RANDOM_SEED)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "# 5. Приводим типы (для TargetEncoder)\n",
    "# Используем .loc для явного указания\n",
    "X_train.loc[:, 'zipcode'] = X_train['zipcode'].astype(str)\n",
    "X_test.loc[:, 'zipcode'] = X_test['zipcode'].astype(str)\n",
    "\n",
    "print(f\"Данные разделены. Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "# НАСТРОЙКА ПРЕПРОЦЕССОРА (Один на всех)\n",
    "\n",
    "# Группировка признаков\n",
    "numeric_features = ['sqft', 'lotsize', 'beds', 'baths', 'year_built', \n",
    "                    'schools_count', 'rating_mean', 'distance_mean', 'age',\n",
    "                    'latitude', 'longitude']\n",
    "\n",
    "categorical_features = ['status', 'propertyType', 'heating', 'cooling', 'parking', 'state']\n",
    "high_cardinality_features = ['zipcode']\n",
    "\n",
    "# Фильтр на случай отсутствия колонок\n",
    "numeric_features = [c for c in numeric_features if c in X.columns]\n",
    "categorical_features = [c for c in categorical_features if c in X.columns]\n",
    "high_cardinality_features = [c for c in high_cardinality_features if c in X.columns]\n",
    "\n",
    "# Создаем трансформеры\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler()) \n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='other')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "target_enc_transformer = Pipeline(steps=[\n",
    "    ('target_enc', TargetEncoder()) \n",
    "])\n",
    "\n",
    "# Глобальный препроцессор\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('target', target_enc_transformer, high_cardinality_features)\n",
    "    ],\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "print(\"Препроцессор готов к использованию.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline модель с использованием HistGradientBoostingRegressor\n",
    "\n",
    "В качестве Baseline мы используем HistGradientBoostingRegressor. Это современная, быстрая реализация бустинга в Sklearn (аналог LightGBM), которая обучается в разы быстрее классического GradientBoosting и лучше подходит для нашего объема данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель\n",
    "start_time = time.time()  # <--- ЗАПУСК ТАЙМЕРА\n",
    "\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', HistGradientBoostingRegressor(max_iter=500, random_state=RANDOM_SEED))\n",
    "])\n",
    "\n",
    "# Обучение\n",
    "print(\"Обучение модели через Pipeline (HistGradientBoostingRegressor)...\")\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Предсказание\n",
    "y_test_pred = model_pipeline.predict(X_test)\n",
    "y_train_pred = model_pipeline.predict(X_train)\n",
    "\n",
    "end_time = time.time()    # <--- ОСТАНОВКА ТАЙМЕРА\n",
    "elapsed_time = end_time - start_time\n",
    "# Вывод метрик\n",
    "metrics_func(y_train, y_train_pred, y_test, y_test_pred, model_name=\"PIPELINE HistGradientBoostingRegressor (Baseline)\", exec_time=elapsed_time)\n",
    "\n",
    "# Сохранение в файл\n",
    "joblib.dump(model_pipeline, './models/histgradientboosting.pkl', compress=3)\n",
    "print(\"Модель 'histgradientboosting.pkl' успешно сохранена.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline c подбором гиперпараметров для модели XGBoost\n",
    "\n",
    "Предпринимаем попытку подобрать гиперпараметры для модели XGBoost с помощью Optuna (быстрее и эффективнее, чем GridSearch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # 1. Параметры для перебора (XGBoost)\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500, 2000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        # Фиксированные параметры\n",
    "        'tree_method': 'hist',\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'random_state': RANDOM_SEED,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    # 2. Создаем пайплайн с этими параметрами\n",
    "    trial_model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', XGBRegressor(**param))\n",
    "    ])\n",
    "    \n",
    "    # 3. Делаем подвыборку (20% от данных), чтобы поиск шел быстро\n",
    "    # Stratify не нужен для регрессии\n",
    "    X_train_sub, _, y_train_sub, _ = train_test_split(\n",
    "        X_train, y_train, train_size=0.2, random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    # Сбрасываем индексы, чтобы TargetEncoder не ругался на несовпадение\n",
    "    X_train_sub = X_train_sub.reset_index(drop=True)\n",
    "    y_train_sub = y_train_sub.reset_index(drop=True)\n",
    "    \n",
    "    # 4. Обучение и оценка\n",
    "    trial_model.fit(X_train_sub, y_train_sub)\n",
    "    \n",
    "    # Предсказываем на всем тестовом наборе (X_test), чтобы оценка была честной\n",
    "    preds = trial_model.predict(X_test)\n",
    "    \n",
    "    return r2_score(y_test, preds)\n",
    "\n",
    "start_time = time.time()  # <--- ЗАПУСК ТАЙМЕРА\n",
    "print(\"Запуск оптимизации гиперпараметров (Optuna)...\")\n",
    "# Создаем study и запускаем\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=15) # 15 попыток \n",
    "\n",
    "print(f\"\\nЛучшие гиперпараметры (Optuna): {study.best_params}\")\n",
    "print(f\"\\nЛучший R2 на тесте подбора гиперпараметров (Optuna): {study.best_value}\")\n",
    "logging.info(f\"\\nЛучшие гиперпараметры (Optuna): {study.best_params}\")\n",
    "logging.info(f\"\\nЛучший R2 на тесте подбора гиперпараметров (Optuna): {study.best_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение модели XGBoost с лучшими гиперпараметрами, полученными с помощью Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Создание финального пайплайна XGBoost с лучшими гиперпараметрами, полученными с помощью Optuna\")\n",
    "\n",
    "# 1. Берем лучшие параметры из исследования Optuna\n",
    "final_params = study.best_params.copy()\n",
    "\n",
    "# 2. Добавляем технические параметры для стабильности\n",
    "final_params.update({\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu', # Используем GPU если есть\n",
    "    'random_state': RANDOM_SEED,\n",
    "    'verbosity': 0\n",
    "})\n",
    "\n",
    "# 3. Собираем финальный Пайплайн\n",
    "# Используем тот же preprocessor, что и везде\n",
    "tuned_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', XGBRegressor(**final_params))\n",
    "])\n",
    "\n",
    "# 4. Обучаем на X_train\n",
    "tuned_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 5. Проверка метрик\n",
    "y_test_pred_tuned = tuned_pipeline.predict(X_test)\n",
    "y_train_pred_tuned = tuned_pipeline.predict(X_train)\n",
    "\n",
    "end_time = time.time()    # <--- ОСТАНОВКА ТАЙМЕРА\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "metrics_func(y_train, y_train_pred_tuned, y_test, y_test_pred_tuned, model_name='PIPELINE OPTUNA TUNED XGBOOST', exec_time=elapsed_time)\n",
    "\n",
    "# # 6. Сохранение в файл\n",
    "joblib.dump(tuned_pipeline, './models/tuned_xgboost.pkl', compress=3)\n",
    "print(\"Модель 'tuned_xgboost.pkl' успешно сохранена.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline модель с использованием StackingRegressor (rf, Catboost и ridge)\n",
    "\n",
    "Экспериментальный вариант с использованием StackingRegressor на GPU (GPU-ACCELERATED STACKING - Только для машин с NVIDIA GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU:\n",
    "    start_time = time.time()  # <--- ЗАПУСК ТАЙМЕРА\n",
    "    print(\"Запускаем усиленный стекинг на GPU (rf, Catboost и ridge).\")\n",
    "    # 1. Определяем базовые модели\n",
    "    # RandomForest: работает на CPU, используем все ядра (n_jobs=-1)\n",
    "    # CatBoost: работает на GPU, что ускорит бустинг в 20-50 раз\n",
    "    estimators = [\n",
    "        ('rf', RandomForestRegressor(n_estimators=100, max_depth=10, random_state=RANDOM_SEED, n_jobs=SAFE_N_JOBS)),\n",
    "        \n",
    "        ('cb', CatBoostRegressor(\n",
    "            iterations=1000,       # Аналог n_estimators, для GPU можно ставить больше (стандарт 1000)\n",
    "            depth=6,               # Аналог max_depth\n",
    "            learning_rate=0.1,     # Стандартная скорость обучения\n",
    "            random_seed=RANDOM_SEED,\n",
    "            task_type=\"GPU\",       # <--- ВКЛЮЧАЕМ ВИДЕОКАРТУ\n",
    "            devices='0',           # Используем первую видеокарту\n",
    "            verbose=0              # Отключаем вывод логов обучения в консоль\n",
    "        )),\n",
    "        \n",
    "        ('ridge', Ridge())\n",
    "    ]\n",
    "\n",
    "    # 2. Создаем Стекинг\n",
    "    # n_jobs=-1 запускает обучение фолдов параллельно. \n",
    "    stacking_regressor = StackingRegressor(\n",
    "        estimators=estimators,\n",
    "        final_estimator=LinearRegression(),\n",
    "        n_jobs=1,  # GPU параллелит вычисления, лишние потоки CPU не нужны\n",
    "        passthrough=False \n",
    "    )\n",
    "\n",
    "    # 3. Пайплайн\n",
    "    model_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', stacking_regressor)\n",
    "    ])\n",
    "\n",
    "    # 4. Обучение\n",
    "    # Мониторинг в терминале Linux: 'nvidia-smi -l 1'\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # 5. Предсказание\n",
    "    y_test_pred = model_pipeline.predict(X_test)\n",
    "    y_train_pred = model_pipeline.predict(X_train)\n",
    "    end_time = time.time()    # <--- ОСТАНОВКА ТАЙМЕРА\n",
    "    elapsed_time = end_time - start_time\n",
    "    # 6. Метрики\n",
    "    metrics_func(y_train, y_train_pred, y_test, y_test_pred, model_name='PIPELINE STACKING RF CatBoost(GPU) RIDGE', exec_time=elapsed_time)\n",
    "\n",
    "    # 7. Сохранение\n",
    "    joblib.dump(model_pipeline, './models/stacking_rf_catboost_ridge.pkl', compress=3)\n",
    "    print(\"Модель 'stacking_rf_catboost_ridge.pkl' успешно сохранена.\")\n",
    "else:\n",
    "    print(\"GPU не найдена или расчет на GPU отключен пользователем. Пропускаем блок Pipeline STACKING RF CatBoost(GPU) RIDGE.\")\n",
    "\n",
    "# =================================================================\n",
    "# MODEL: PIPELINE STACKING RF CatBoost(GPU) RIDGE\n",
    "# -----------------------------------------------------------------\n",
    "# Metric     |                Train |                 Test\n",
    "# -----------------------------------------------------------------\n",
    "# MAE        |           115,784.87 |           120,102.04\n",
    "# MSE        |       44,724,064,117 |       49,404,044,654\n",
    "# R2         |               0.8070 |               0.7869\n",
    "# -----------------------------------------------------------------\n",
    "# Time       |                                 2 min 9 sec\n",
    "# =================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline модели с использованием StackingRegressor из эстиматоров RandomForestRegressor + HistGradientBoostingRegressor + Ridge\n",
    "\n",
    "Облегченный вариант модели стекинга с использованием RandomForestRegressor, HistGradientBoostingRegressor и Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  # <--- ЗАПУСК ТАЙМЕРА\n",
    "\n",
    "# HistGradientBoostingRegressor\n",
    "estimators = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=50, max_depth=10, random_state=RANDOM_SEED, n_jobs=SAFE_N_JOBS)), # Уменьшили деревья до 50 для скорости\n",
    "    ('hgb', HistGradientBoostingRegressor(random_state=RANDOM_SEED)), \n",
    "    ('ridge', Ridge())\n",
    "]\n",
    "\n",
    "stacking_regressor = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=Ridge(),\n",
    "    cv=3, \n",
    "    n_jobs=SAFE_N_JOBS # Параллелим процесс\n",
    ")\n",
    "\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', stacking_regressor)\n",
    "])\n",
    "\n",
    "print(\"Обучение StackingRegressor (RandomForest + HistGradientBoosting + Ridge)...\")\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Предсказание\n",
    "y_test_pred = model_pipeline.predict(X_test)\n",
    "y_train_pred = model_pipeline.predict(X_train)\n",
    "end_time = time.time()    # <--- ОСТАНОВКА ТАЙМЕРА\n",
    "elapsed_time = end_time - start_time\n",
    "# Вывод метрик\n",
    "metrics_func(y_train, y_train_pred, y_test, y_test_pred, model_name='PIPELINE STACKING RF+histGB+Ridge', exec_time=elapsed_time)\n",
    "\n",
    "# Сохранение в файл\n",
    "joblib.dump(model_pipeline, './models/stacking_rf_histgb_ridge.pkl', compress=3)\n",
    "print(\"Модель 'stacking_rf_histgb_ridge.pkl' успешно сохранена.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 4 - с использованием StackingRegressor из Triple XGBoost на GPU\n",
    "\n",
    "Экспериментальный вариант с использованием StackingRegressor Triple XGBoost на GPU (GPU-ACCELERATED STACKING - Только для машин с NVIDIA GPU)\n",
    "\n",
    "|Модель| Описание|\n",
    "|--|---|\n",
    "| xgb_deep | Глубокая, очень медленная, ловит нюансы|\n",
    "| xgb_medium | Средняя глубина, баланс|\n",
    "| xgb_shallow | Поверхностная, быстрая, ловит тренды|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Экспериментальный вариант с использованием StackingRegressor на GPU\n",
    "\n",
    "if USE_GPU:\n",
    "    start_time = time.time()  # <--- ЗАПУСК ТАЙМЕРА\n",
    "    print(\"Запускаем усиленный стекинг Triple XGBoost\")\n",
    "    \n",
    "    # Чтобы стекинг работал, модели должны быть разными.\n",
    "    # Создадим 3 вариации XGBoost:\n",
    "    \n",
    "# --- МОДЕЛИ \"ТЯЖЕЛОЙ АРТИЛЛЕРИИ\" ---\n",
    "    \n",
    "    # 1. \"Снайпер\" (Глубокая, очень медленная, ловит нюансы)\n",
    "    # Уменьшаем LR до 0.005, увеличиваем деревья до 7000\n",
    "    xgb_deep = XGBRegressor(\n",
    "        n_estimators=7000,\n",
    "        learning_rate=0.005,     \n",
    "        max_depth=10,            \n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "        reg_lambda=1,            # L2 регуляризация от переобучения\n",
    "        tree_method='hist',      \n",
    "        device='cuda',           \n",
    "        random_state=RANDOM_SEED,\n",
    "        verbosity=0\n",
    "    )\n",
    "\n",
    "    # 2. \"Тактик\" (Средняя глубина, баланс) - НОВАЯ МОДЕЛЬ\n",
    "    # Закрывает \"пробел\" между глубокой и мелкой моделями\n",
    "    xgb_medium = XGBRegressor(\n",
    "        n_estimators=6000,\n",
    "        learning_rate=0.01,      \n",
    "        max_depth=6,             # Средняя глубина\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        tree_method='hist',      \n",
    "        device='cuda',           \n",
    "        random_state=RANDOM_SEED+50,         # Другой seed для разнообразия\n",
    "        verbosity=0\n",
    "    )\n",
    "\n",
    "    # 3. \"Сканер\" (Поверхностная, быстрая, ловит тренды)\n",
    "    # Еще больше деревьев, но маленькая глубина\n",
    "    xgb_shallow = XGBRegressor(\n",
    "        n_estimators=8000,\n",
    "        learning_rate=0.02,      \n",
    "        max_depth=3,             # Совсем неглубокие деревья (меньше переобучения)\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        tree_method='hist',      \n",
    "        device='cuda',           \n",
    "        random_state=RANDOM_SEED+100,        \n",
    "        verbosity=0\n",
    "    )\n",
    "\n",
    "    # Список моделей (3)\n",
    "    estimators_gpu = [\n",
    "        ('xgb_deep', xgb_deep),\n",
    "        ('xgb_medium', xgb_medium),\n",
    "        ('xgb_shallow', xgb_shallow)\n",
    "    ]\n",
    "    \n",
    "    # ... далее создание StackingRegressor без изменений ...\n",
    "    # 3. Создание Стекинга\n",
    "    stacking_gpu = StackingRegressor(\n",
    "        estimators=estimators_gpu,\n",
    "        final_estimator=Ridge(),\n",
    "        cv=5,       \n",
    "        n_jobs=1,   # GPU параллелит вычисления, лишние потоки CPU не нужны\n",
    "        passthrough=False \n",
    "    )\n",
    "\n",
    "    # 4. Сборка пайплайна\n",
    "    gpu_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor), \n",
    "        ('regressor', stacking_gpu)\n",
    "    ])\n",
    "\n",
    "    # 5. Обучение\n",
    "    gpu_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # 6. Оценка\n",
    "    y_test_pred_gpu = gpu_pipeline.predict(X_test)\n",
    "    y_train_pred_gpu = gpu_pipeline.predict(X_train)\n",
    "    end_time = time.time()    # <--- ОСТАНОВКА ТАЙМЕРА\n",
    "    elapsed_time = end_time - start_time\n",
    "    metrics_func(y_train, y_train_pred_gpu, y_test, y_test_pred_gpu, model_name='PIPELINE STACKING Triple XGBoost', exec_time=elapsed_time)\n",
    "\n",
    "    # Сохранение в файл\n",
    "    joblib.dump(gpu_pipeline, './models/stacking_triple_xgboost.pkl', compress=3)\n",
    "    print(\"Модель 'stacking_triple_xgboost.pkl' успешно сохранена.\")\n",
    "else:\n",
    "    print(\"GPU не найдена или расчет на GPU отключен пользователем. Пропускаем блок ускоренного стекинга.\")\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# MODEL: PIPELINE STACKING Triple XGBoost\n",
    "# -----------------------------------------------------------------\n",
    "# Metric     |                Train |                 Test\n",
    "# -----------------------------------------------------------------\n",
    "# MAE        |            64,658.78 |            96,222.70\n",
    "# MSE        |       14,372,935,342 |       37,206,576,062\n",
    "# R2         |               0.9380 |               0.8395\n",
    "# -----------------------------------------------------------------\n",
    "# Time       |                               11 min 21 sec\n",
    "# =================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формируем сводную таблицу с итогами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Формируем DataFrame из накопленных данных\n",
    "df_results = pd.DataFrame(EXPERIMENTS_DATA)\n",
    "\n",
    "if not df_results.empty:\n",
    "    # Сортируем\n",
    "    df_results = df_results.sort_values(by='R2 Score (Test)', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    print(\"=== ИТОГОВАЯ ТАБЛИЦА ЭКСПЕРИМЕНТОВ ===\")\n",
    "    \n",
    "    # Определяем колонки для раскраски (если они есть в датафрейме)\n",
    "    r2_cols = [c for c in df_results.columns if 'R2' in c]\n",
    "    error_cols = [c for c in df_results.columns if 'MAE' in c or 'MSE' in c]\n",
    "\n",
    "    # Применяем стили\n",
    "    # R2: Больше = Лучше (viridis: Желтый - хорошо, Фиолетовый - плохо)\n",
    "    # Errors: Меньше = Лучше (viridis_r: Желтый - хорошо/мало, Фиолетовый - плохо/много)\n",
    "    styled_results = df_results.style.background_gradient(\n",
    "        cmap='viridis', \n",
    "        subset=r2_cols\n",
    "    ).background_gradient(\n",
    "        cmap='viridis_r', \n",
    "        subset=error_cols\n",
    "    ).format({\n",
    "        # Форматирование чисел для красоты\n",
    "        'R2 Score (Train)': '{:.3f}',\n",
    "        'R2 Score (Test)': '{:.3f}',\n",
    "        'MAE (Train)': '{:,.0f}',\n",
    "        'MAE (Test)': '{:,.0f}',\n",
    "        'MSE (Train)': '{:,.2e}', # Научный формат для огромных чисел\n",
    "        'MSE (Test)': '{:,.2e}'\n",
    "    })\n",
    "\n",
    "    display(styled_results)\n",
    "    \n",
    "    # --- ЗАПИСЬ В ЛОГ (Текстовая таблица) ---\n",
    "    # to_string создает красивое текстовое представление таблицы\n",
    "    table_str = df_results.to_string(index=False, justify='right')\n",
    "    \n",
    "    # Считаем общее время (если TOTAL_START_TIME был определен в начале)\n",
    "    if 'TOTAL_START_TIME' in locals():\n",
    "        total_elapsed = time.time() - TOTAL_START_TIME\n",
    "        mins, secs = divmod(total_elapsed, 60)\n",
    "        total_time_str = f\"{int(mins)} min {int(secs)} sec\"\n",
    "    else:\n",
    "        total_time_str = \"N/A\"\n",
    "    \n",
    "    final_msg = (\n",
    "        f\"\\n{'='*80}\\n\"\n",
    "        f\"FINAL SUMMARY REPORT\\n\"\n",
    "        f\"{'-'*80}\\n\"\n",
    "        f\"{table_str}\\n\"\n",
    "        f\"{'-'*80}\\n\"\n",
    "        f\"Total Execution Time: {total_time_str}\\n\"\n",
    "        f\"{'='*80}\\n\"\n",
    "    )\n",
    "    \n",
    "    logging.info(final_msg)\n",
    "    \n",
    "    # Сохраняем CSV\n",
    "    csv_filename = f\"logs/results_{timestamp}.csv\"\n",
    "    df_results.to_csv(csv_filename, index=False)\n",
    "    print(f\"Отчет сохранен в: {csv_filename}\")\n",
    "\n",
    "else:\n",
    "    logging.warning(\"Нет данных об экспериментах. Таблица пуста.\")\n",
    "\n",
    "# --- ЗАВЕРШЕНИЕ РАБОТЫ ЛОГГЕРА ---\n",
    "def shutdown_logging():\n",
    "    logger = logging.getLogger()\n",
    "    for handler in logger.handlers[:]:\n",
    "        handler.close()\n",
    "        logger.removeHandler(handler)\n",
    "    logging.shutdown()\n",
    "    print(\"Логирование завершено.\")\n",
    "\n",
    "shutdown_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Заключение и выводы\n",
    "\n",
    "Проект прошел путь от базовых экспериментов до построения высокоточной модели оценки недвижимости. Мы протестировали более 15 конфигураций моделей.\n",
    "\n",
    "### 1. Влияние Feature Engineering\n",
    "Ключевым драйвером роста точности стала работа с данными, а не просто перебор алгоритмов:\n",
    "\n",
    "*   **Ограниченность простого отбора признаков:**\n",
    "    Модели, обученные на ограниченном наборе лучших признаков (`Top-16` и `Top-30`), показали результат **$R^2 \\approx 0.60 - 0.76$**. Этого оказалось недостаточно.\n",
    "*   **Эффект Target Encoding:**\n",
    "    Внедрение полных пайплайнов с использованием `TargetEncoder` для почтового индекса (`zipcode`) позволило поднять метрику до **$R^2 \\approx 0.80$** (Baseline).\n",
    "> **Вывод:** Для оценки недвижимости *способ кодирования локации* (превращение индекса в среднюю цену района) важнее, чем сокращение количества признаков.\n",
    "\n",
    "### 2. Сравнение архитектур (Результаты экспериментов)\n",
    "\n",
    "*   **Baseline (HistGradientBoosting):** $R^2 = 0.801$, MAE = \\$116k.\n",
    "    Современная реализация бустинга в sklearn показала достойный результат \"из коробки\", подтвердив качество предобработки данных.\n",
    "*   **Optimization (Tuned XGBoost + Optuna):** $R^2 = 0.820$, MAE = \\$106k.\n",
    "    Автоматический подбор гиперпараметров позволил выжать максимум из одиночной модели, снизив ошибку на \\$10,000 по сравнению с бейзлайном.\n",
    "*   **Ensemble (Stacking):**\n",
    "    *   *CPU-версия (RF + HistGB)* провалилась ($0.76$), доказав, что объединение слабых моделей не дает прироста.\n",
    "    *   *GPU-версия (Triple XGBoost)* дала лучший результат: **$R^2 = 0.840$**, MAE = **\\$96k**.\n",
    "\n",
    "### 3. Итоговая рекомендация для внедрения (Production)\n",
    "\n",
    "Мы рекомендуем к внедрению модель **Tuned XGBoost Pipeline** ($R^2=0.82$, файл `tuned_xgboost.pkl`).\n",
    "\n",
    "**Обоснование:**\n",
    "*   **Баланс качества и скорости:** Уступает сложному стекингу всего 2% точности, но обучается в 10 раз быстрее (1 минута против 11 минут) и потребляет меньше ресурсов.\n",
    "*   **Простота поддержки:** Одиночная модель легче в мониторинге и обновлении, чем ансамбль из 3-х тяжелых моделей.\n",
    "*   **Автономность:** Модель упакована в единый `Pipeline`, который автоматически обрабатывает сырые данные (включая пропуски и новые категории).\n",
    "\n",
    "> *Примечание:* Если бизнес-задача требует борьбы за каждую долю процента точности (например, для высокочастотного трейдинга недвижимостью), можно использовать **GPU Stacking** ($R^2=0.84$), который первым пробил планку ошибки MAE < \\$100k.\n",
    "\n",
    "### 4. Бизнес-результат\n",
    "Разработанная система позволяет оценивать недвижимость со средней абсолютной ошибкой (MAE) около **$96,000 - 106,000**, что при разбросе цен до \\$3M является высоким показателем точности для автоматической оценки (AVM) по всей территории США.\n",
    "\n",
    "\n",
    "### 5. Рекомендации по использованию модели в бизнесе (Business Value)\n",
    "\n",
    "Учитывая метрики модели ($MAE \\approx \\$106k$, $R^2 \\approx 0.82$) и скорость её работы, предлагаем следующие сценарии внедрения разработанного решения в процессы агентства:\n",
    "\n",
    "**1. Инструмент для приоритизации сделок (Lead Scoring)**\n",
    "*   **Сценарий:** Ежедневно в базу попадают тысячи новых объявлений.\n",
    "*   **Решение:** Модель автоматически оценивает каждый новый объект.\n",
    "*   **Действие:** Если `Предсказанная цена` > `Цена листинга` (например, на 20%), объект помечается флагом **\"Potential Deal\"** (Недооценен) и отправляется топ-агентам в первую очередь. Это позволит выкупать выгодные лоты быстрее конкурентов.\n",
    "\n",
    "**2. Сервис экспресс-оценки для привлечения клиентов (Lead Generation)**\n",
    "*   **Сценарий:** Владелец дома хочет узнать рыночную стоимость своего жилья.\n",
    "*   **Решение:** Виджет на сайте агентства \"Оцени свой дом за 1 минуту\". Пользователь вводит параметры (площадь, зип-код, спальни), модель выдает диапазон цен (например, \\$550k $\\pm$ 15%).\n",
    "*   **Ценность:** Сбор контактов (лидов) потенциальных продавцов недвижимости.\n",
    "\n",
    "**3. \"Второе мнение\" при назначении цены (Listing Price Advisor)**\n",
    "*   **Сценарий:** Риелтор договаривается с продавцом о цене выхода на рынок. Продавец часто завышает ожидания.\n",
    "*   **Решение:** Риелтор использует модель как объективный аргумент: *\"Наш ИИ-алгоритм, обученный на 370 000 сделок, показывает, что рыночный коридор для вашего дома — \\$600-650k. Если поставим \\$800k, мы потеряем время\"*.\n",
    "\n",
    "**4. Контроль аномалий и ошибок ввода**\n",
    "*   **Сценарий:** Агент ошибся лишним нулем в площади или цене при заполнении карточки.\n",
    "*   **Решение:** Если предсказание модели отличается от введенной цены более чем на 50%, система блокирует публикацию объявления и просит менеджера проверить данные. Это повысит качество базы данных.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7. Подготовка артефактов для деплоя \n",
    "\n",
    "Автоматически генерируем Data Contract (Контракт данных) данных для разработчиков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Берем 5 случайных строк из теста\n",
    "sample_indices = X_test.sample(5, random_state=42).index\n",
    "sample_X = X_test.loc[sample_indices].copy()\n",
    "sample_y = y_test.loc[sample_indices]\n",
    "\n",
    "# 2. Делаем предсказание для них (чтобы проверить, что модель работает и сравнить с фактом)\n",
    "# Используем наш выбранный пайплайн (например, tuned_pipeline или model_pipeline)\n",
    "current_model = tuned_pipeline \n",
    "sample_preds = current_model.predict(sample_X)\n",
    "\n",
    "# 3. Собираем всё в один DataFrame\n",
    "export_df = sample_X.copy()\n",
    "export_df['Actual_Price'] = sample_y\n",
    "export_df['Predicted_Price'] = sample_preds\n",
    "\n",
    "# 4. Сохраняем в CSV (это пойдет в Google Таблицы как пример)\n",
    "csv_filename = 'deployment/model_input_examples.csv'\n",
    "# Создадим папку, если нет\n",
    "if not os.path.exists('deployment'):\n",
    "    os.makedirs('deployment')\n",
    "    \n",
    "export_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Примеры данных сохранены в: {csv_filename}\")\n",
    "\n",
    "# 5. Генерируем готовый словарь для client.py\n",
    "# Берем первую строку\n",
    "single_row_dict = sample_X.iloc[0].to_dict()\n",
    "\n",
    "# Конвертируем numpy типы в стандартные python (иначе json.dumps упадет)\n",
    "def convert_types(obj):\n",
    "    if isinstance(obj, (np.int64, np.int32)): return int(obj)\n",
    "    if isinstance(obj, (np.float64, np.float32)): return float(obj)\n",
    "    return obj\n",
    "\n",
    "clean_dict = {k: convert_types(v) for k, v in single_row_dict.items()}\n",
    "\n",
    "print(\"\\nСкопируйте этот словарь в client.py (переменная sample_data):\")\n",
    "print(\"-\" * 50)\n",
    "print(json.dumps(clean_dict, indent=4))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 6. Выводим список колонок для сверки\n",
    "print(\"\\nСписок колонок для Google Sheets (по порядку):\")\n",
    "print(list(sample_X.columns))\n",
    "\n",
    "# 1. Берем 5 случайных строк из теста\n",
    "sample_indices = X_test.sample(5, random_state=42).index\n",
    "sample_X = X_test.loc[sample_indices].copy()\n",
    "sample_y = y_test.loc[sample_indices]\n",
    "\n",
    "# 2. Делаем предсказание для них (чтобы проверить, что модель работает и сравнить с фактом)\n",
    "# Используем наш лучший пайплайн (например, tuned_pipeline или model_pipeline)\n",
    "current_model = tuned_pipeline \n",
    "sample_preds = current_model.predict(sample_X)\n",
    "\n",
    "# 3. Собираем всё в один DataFrame\n",
    "export_df = sample_X.copy()\n",
    "export_df['Actual_Price'] = sample_y\n",
    "export_df['Predicted_Price'] = sample_preds\n",
    "\n",
    "# 4. Сохраняем в CSV (это пойдет в Google Таблицы как пример)\n",
    "csv_filename = 'deployment/model_input_examples.csv'\n",
    "# Создадим папку, если нет\n",
    "if not os.path.exists('deployment'):\n",
    "    os.makedirs('deployment')\n",
    "    \n",
    "export_df.to_csv(csv_filename, index=False)\n",
    "print(f\"Примеры данных сохранены в: {csv_filename}\")\n",
    "\n",
    "# 5. Генерируем готовый словарь для client.py\n",
    "# Берем первую строку\n",
    "single_row_dict = sample_X.iloc[0].to_dict()\n",
    "\n",
    "# Конвертируем numpy типы в стандартные python (иначе json.dumps упадет)\n",
    "def convert_types(obj):\n",
    "    if isinstance(obj, (np.int64, np.int32)): return int(obj)\n",
    "    if isinstance(obj, (np.float64, np.float32)): return float(obj)\n",
    "    return obj\n",
    "\n",
    "clean_dict = {k: convert_types(v) for k, v in single_row_dict.items()}\n",
    "\n",
    "print(\"\\nСкопируйте этот словарь в client.py (переменная sample_data):\")\n",
    "print(\"-\" * 50)\n",
    "print(json.dumps(clean_dict, indent=4))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 6. Выводим список колонок для сверки\n",
    "print(\"\\nСписок колонок для Google Sheets (по порядку):\")\n",
    "print(list(sample_X.columns))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
